{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias para manejo de datos\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "# Librerias para preprocesamiento y visualizacion de datos\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Librerias para implementar redes neuronales\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando informacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>production_date</th>\n",
       "      <th>runtime_minutes</th>\n",
       "      <th>director</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_votes</th>\n",
       "      <th>approval_index</th>\n",
       "      <th>prod_budget</th>\n",
       "      <th>domestic_gross</th>\n",
       "      <th>worldwide_gross</th>\n",
       "      <th>...</th>\n",
       "      <th>music</th>\n",
       "      <th>musical</th>\n",
       "      <th>mystery</th>\n",
       "      <th>news</th>\n",
       "      <th>romance</th>\n",
       "      <th>sci-fi</th>\n",
       "      <th>sport</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar: The Way of Water</td>\n",
       "      <td>1670544000</td>\n",
       "      <td>192.0</td>\n",
       "      <td>664</td>\n",
       "      <td>7.8</td>\n",
       "      <td>277543.0</td>\n",
       "      <td>7.061101</td>\n",
       "      <td>460000000</td>\n",
       "      <td>667830256</td>\n",
       "      <td>2265935552</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: On Stranger Tides</td>\n",
       "      <td>1305849600</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>6.6</td>\n",
       "      <td>533763.0</td>\n",
       "      <td>6.272064</td>\n",
       "      <td>379000000</td>\n",
       "      <td>241071802</td>\n",
       "      <td>1045713802</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avengers: Age of Ultron</td>\n",
       "      <td>1429660800</td>\n",
       "      <td>141.0</td>\n",
       "      <td>899</td>\n",
       "      <td>7.3</td>\n",
       "      <td>870573.0</td>\n",
       "      <td>7.214013</td>\n",
       "      <td>365000000</td>\n",
       "      <td>459005868</td>\n",
       "      <td>1395316979</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Justice League</td>\n",
       "      <td>1510531200</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1829</td>\n",
       "      <td>6.1</td>\n",
       "      <td>456977.0</td>\n",
       "      <td>5.717212</td>\n",
       "      <td>300000000</td>\n",
       "      <td>229024295</td>\n",
       "      <td>655945209</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>1444089600</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1531</td>\n",
       "      <td>6.8</td>\n",
       "      <td>445461.0</td>\n",
       "      <td>6.375644</td>\n",
       "      <td>300000000</td>\n",
       "      <td>200074175</td>\n",
       "      <td>879077344</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  production_date  \\\n",
       "0                     Avatar: The Way of Water       1670544000   \n",
       "1  Pirates of the Caribbean: On Stranger Tides       1305849600   \n",
       "2                      Avengers: Age of Ultron       1429660800   \n",
       "3                               Justice League       1510531200   \n",
       "4                                      Spectre       1444089600   \n",
       "\n",
       "   runtime_minutes  director  average_rating  num_votes  approval_index  \\\n",
       "0            192.0       664             7.8   277543.0        7.061101   \n",
       "1            137.0      1445             6.6   533763.0        6.272064   \n",
       "2            141.0       899             7.3   870573.0        7.214013   \n",
       "3            120.0      1829             6.1   456977.0        5.717212   \n",
       "4            148.0      1531             6.8   445461.0        6.375644   \n",
       "\n",
       "   prod_budget  domestic_gross  worldwide_gross  ...  music  musical  mystery  \\\n",
       "0    460000000       667830256       2265935552  ...      0        0        0   \n",
       "1    379000000       241071802       1045713802  ...      0        0        0   \n",
       "2    365000000       459005868       1395316979  ...      0        0        0   \n",
       "3    300000000       229024295        655945209  ...      0        0        0   \n",
       "4    300000000       200074175        879077344  ...      0        0        0   \n",
       "\n",
       "   news  romance  sci-fi  sport  thriller  war  western  \n",
       "0     0        0       0      0         0    0        0  \n",
       "1     0        0       0      0         0    0        0  \n",
       "2     0        0       1      0         0    0        0  \n",
       "3     0        0       0      0         0    0        0  \n",
       "4     0        0       0      0         1    0        0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/dataset_clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x2B2E68440"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[['prod_budget', 'domestic_gross', 'worldwide_gross', 'director', 'runtime_minutes', 'approval_index', 'num_votes']]\n",
    "y = data[['average_rating']]\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X,\n",
    "  y,\n",
    "  test_size=0.20\n",
    ")\n",
    "train_labels, train_samples = shuffle(y_train,X_train )\n",
    "\n",
    "seed_value = 2302\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "sklearn.utils.check_random_state(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3837662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_92 (Dense)            (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 128)               2176      \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15873 (62.00 KB)\n",
      "Trainable params: 15329 (59.88 KB)\n",
      "Non-trainable params: 544 (2.12 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(7,), activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, ) \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 - 1s - loss: 37.8510 - mean_squared_error: 37.8510 - val_loss: 30.2548 - val_mean_squared_error: 30.2548 - 586ms/epoch - 9ms/step\n",
      "Epoch 2/200\n",
      "65/65 - 0s - loss: 25.6909 - mean_squared_error: 25.6909 - val_loss: 14.7027 - val_mean_squared_error: 14.7027 - 81ms/epoch - 1ms/step\n",
      "Epoch 3/200\n",
      "65/65 - 0s - loss: 11.1424 - mean_squared_error: 11.1424 - val_loss: 3.6316 - val_mean_squared_error: 3.6316 - 77ms/epoch - 1ms/step\n",
      "Epoch 4/200\n",
      "65/65 - 0s - loss: 2.9880 - mean_squared_error: 2.9880 - val_loss: 1.1174 - val_mean_squared_error: 1.1174 - 75ms/epoch - 1ms/step\n",
      "Epoch 5/200\n",
      "65/65 - 0s - loss: 1.2569 - mean_squared_error: 1.2569 - val_loss: 1.0753 - val_mean_squared_error: 1.0753 - 73ms/epoch - 1ms/step\n",
      "Epoch 6/200\n",
      "65/65 - 0s - loss: 1.0877 - mean_squared_error: 1.0877 - val_loss: 1.0347 - val_mean_squared_error: 1.0347 - 75ms/epoch - 1ms/step\n",
      "Epoch 7/200\n",
      "65/65 - 0s - loss: 1.0661 - mean_squared_error: 1.0661 - val_loss: 1.0035 - val_mean_squared_error: 1.0035 - 75ms/epoch - 1ms/step\n",
      "Epoch 8/200\n",
      "65/65 - 0s - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 0.9432 - val_mean_squared_error: 0.9432 - 81ms/epoch - 1ms/step\n",
      "Epoch 9/200\n",
      "65/65 - 0s - loss: 1.0359 - mean_squared_error: 1.0359 - val_loss: 1.0135 - val_mean_squared_error: 1.0135 - 78ms/epoch - 1ms/step\n",
      "Epoch 10/200\n",
      "65/65 - 0s - loss: 1.0476 - mean_squared_error: 1.0476 - val_loss: 1.0136 - val_mean_squared_error: 1.0136 - 78ms/epoch - 1ms/step\n",
      "Epoch 11/200\n",
      "65/65 - 0s - loss: 1.0295 - mean_squared_error: 1.0295 - val_loss: 0.9801 - val_mean_squared_error: 0.9801 - 76ms/epoch - 1ms/step\n",
      "Epoch 12/200\n",
      "65/65 - 0s - loss: 1.0140 - mean_squared_error: 1.0140 - val_loss: 0.9964 - val_mean_squared_error: 0.9964 - 77ms/epoch - 1ms/step\n",
      "Epoch 13/200\n",
      "65/65 - 0s - loss: 0.9972 - mean_squared_error: 0.9972 - val_loss: 0.9521 - val_mean_squared_error: 0.9521 - 77ms/epoch - 1ms/step\n",
      "Epoch 14/200\n",
      "65/65 - 0s - loss: 1.0189 - mean_squared_error: 1.0189 - val_loss: 0.9721 - val_mean_squared_error: 0.9721 - 79ms/epoch - 1ms/step\n",
      "Epoch 15/200\n",
      "65/65 - 0s - loss: 0.9938 - mean_squared_error: 0.9938 - val_loss: 1.0015 - val_mean_squared_error: 1.0015 - 78ms/epoch - 1ms/step\n",
      "Epoch 16/200\n",
      "65/65 - 0s - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 0.9560 - val_mean_squared_error: 0.9560 - 84ms/epoch - 1ms/step\n",
      "Epoch 17/200\n",
      "65/65 - 0s - loss: 0.9861 - mean_squared_error: 0.9861 - val_loss: 1.0411 - val_mean_squared_error: 1.0411 - 80ms/epoch - 1ms/step\n",
      "Epoch 18/200\n",
      "65/65 - 0s - loss: 1.0195 - mean_squared_error: 1.0195 - val_loss: 0.9387 - val_mean_squared_error: 0.9387 - 76ms/epoch - 1ms/step\n",
      "Epoch 19/200\n",
      "65/65 - 0s - loss: 0.9845 - mean_squared_error: 0.9845 - val_loss: 1.0373 - val_mean_squared_error: 1.0373 - 81ms/epoch - 1ms/step\n",
      "Epoch 20/200\n",
      "65/65 - 0s - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 0.9664 - val_mean_squared_error: 0.9664 - 81ms/epoch - 1ms/step\n",
      "Epoch 21/200\n",
      "65/65 - 0s - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 0.9789 - val_mean_squared_error: 0.9789 - 79ms/epoch - 1ms/step\n",
      "Epoch 22/200\n",
      "65/65 - 0s - loss: 1.0016 - mean_squared_error: 1.0016 - val_loss: 0.9603 - val_mean_squared_error: 0.9603 - 73ms/epoch - 1ms/step\n",
      "Epoch 23/200\n",
      "65/65 - 0s - loss: 0.9565 - mean_squared_error: 0.9565 - val_loss: 0.9855 - val_mean_squared_error: 0.9855 - 79ms/epoch - 1ms/step\n",
      "Epoch 24/200\n",
      "65/65 - 0s - loss: 0.9868 - mean_squared_error: 0.9868 - val_loss: 0.9567 - val_mean_squared_error: 0.9567 - 76ms/epoch - 1ms/step\n",
      "Epoch 25/200\n",
      "65/65 - 0s - loss: 0.9739 - mean_squared_error: 0.9739 - val_loss: 0.9953 - val_mean_squared_error: 0.9953 - 77ms/epoch - 1ms/step\n",
      "Epoch 26/200\n",
      "65/65 - 0s - loss: 0.9503 - mean_squared_error: 0.9503 - val_loss: 0.9698 - val_mean_squared_error: 0.9698 - 82ms/epoch - 1ms/step\n",
      "Epoch 27/200\n",
      "65/65 - 0s - loss: 0.9322 - mean_squared_error: 0.9322 - val_loss: 1.1331 - val_mean_squared_error: 1.1331 - 77ms/epoch - 1ms/step\n",
      "Epoch 28/200\n",
      "65/65 - 0s - loss: 0.9121 - mean_squared_error: 0.9121 - val_loss: 1.0027 - val_mean_squared_error: 1.0027 - 74ms/epoch - 1ms/step\n",
      "Epoch 29/200\n",
      "65/65 - 0s - loss: 0.8068 - mean_squared_error: 0.8068 - val_loss: 1.4007 - val_mean_squared_error: 1.4007 - 102ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "65/65 - 0s - loss: 0.7884 - mean_squared_error: 0.7884 - val_loss: 1.3415 - val_mean_squared_error: 1.3415 - 78ms/epoch - 1ms/step\n",
      "Epoch 31/200\n",
      "65/65 - 0s - loss: 0.7677 - mean_squared_error: 0.7677 - val_loss: 1.3338 - val_mean_squared_error: 1.3338 - 77ms/epoch - 1ms/step\n",
      "Epoch 32/200\n",
      "65/65 - 0s - loss: 0.7836 - mean_squared_error: 0.7836 - val_loss: 1.4155 - val_mean_squared_error: 1.4155 - 78ms/epoch - 1ms/step\n",
      "Epoch 33/200\n",
      "65/65 - 0s - loss: 0.7684 - mean_squared_error: 0.7684 - val_loss: 0.9911 - val_mean_squared_error: 0.9911 - 76ms/epoch - 1ms/step\n",
      "Epoch 34/200\n",
      "65/65 - 0s - loss: 0.7653 - mean_squared_error: 0.7653 - val_loss: 0.9036 - val_mean_squared_error: 0.9036 - 77ms/epoch - 1ms/step\n",
      "Epoch 35/200\n",
      "65/65 - 0s - loss: 0.7506 - mean_squared_error: 0.7506 - val_loss: 0.7680 - val_mean_squared_error: 0.7680 - 75ms/epoch - 1ms/step\n",
      "Epoch 36/200\n",
      "65/65 - 0s - loss: 0.7676 - mean_squared_error: 0.7676 - val_loss: 0.8585 - val_mean_squared_error: 0.8585 - 73ms/epoch - 1ms/step\n",
      "Epoch 37/200\n",
      "65/65 - 0s - loss: 0.7341 - mean_squared_error: 0.7341 - val_loss: 0.7640 - val_mean_squared_error: 0.7640 - 77ms/epoch - 1ms/step\n",
      "Epoch 38/200\n",
      "65/65 - 0s - loss: 0.7401 - mean_squared_error: 0.7401 - val_loss: 0.7563 - val_mean_squared_error: 0.7563 - 74ms/epoch - 1ms/step\n",
      "Epoch 39/200\n",
      "65/65 - 0s - loss: 0.7492 - mean_squared_error: 0.7492 - val_loss: 0.7574 - val_mean_squared_error: 0.7574 - 73ms/epoch - 1ms/step\n",
      "Epoch 40/200\n",
      "65/65 - 0s - loss: 0.7451 - mean_squared_error: 0.7451 - val_loss: 0.7802 - val_mean_squared_error: 0.7802 - 72ms/epoch - 1ms/step\n",
      "Epoch 41/200\n",
      "65/65 - 0s - loss: 0.7452 - mean_squared_error: 0.7452 - val_loss: 0.8735 - val_mean_squared_error: 0.8735 - 74ms/epoch - 1ms/step\n",
      "Epoch 42/200\n",
      "65/65 - 0s - loss: 0.7338 - mean_squared_error: 0.7338 - val_loss: 0.9748 - val_mean_squared_error: 0.9748 - 79ms/epoch - 1ms/step\n",
      "Epoch 43/200\n",
      "65/65 - 0s - loss: 0.7382 - mean_squared_error: 0.7382 - val_loss: 0.8431 - val_mean_squared_error: 0.8431 - 76ms/epoch - 1ms/step\n",
      "Epoch 44/200\n",
      "65/65 - 0s - loss: 0.7375 - mean_squared_error: 0.7375 - val_loss: 0.7249 - val_mean_squared_error: 0.7249 - 77ms/epoch - 1ms/step\n",
      "Epoch 45/200\n",
      "65/65 - 0s - loss: 0.7373 - mean_squared_error: 0.7373 - val_loss: 0.7461 - val_mean_squared_error: 0.7461 - 74ms/epoch - 1ms/step\n",
      "Epoch 46/200\n",
      "65/65 - 0s - loss: 0.7303 - mean_squared_error: 0.7303 - val_loss: 0.6887 - val_mean_squared_error: 0.6887 - 74ms/epoch - 1ms/step\n",
      "Epoch 47/200\n",
      "65/65 - 0s - loss: 0.7138 - mean_squared_error: 0.7138 - val_loss: 0.8036 - val_mean_squared_error: 0.8036 - 74ms/epoch - 1ms/step\n",
      "Epoch 48/200\n",
      "65/65 - 0s - loss: 0.7290 - mean_squared_error: 0.7290 - val_loss: 0.7286 - val_mean_squared_error: 0.7286 - 79ms/epoch - 1ms/step\n",
      "Epoch 49/200\n",
      "65/65 - 0s - loss: 0.7309 - mean_squared_error: 0.7309 - val_loss: 0.8371 - val_mean_squared_error: 0.8371 - 80ms/epoch - 1ms/step\n",
      "Epoch 50/200\n",
      "65/65 - 0s - loss: 0.7311 - mean_squared_error: 0.7311 - val_loss: 0.6978 - val_mean_squared_error: 0.6978 - 79ms/epoch - 1ms/step\n",
      "Epoch 51/200\n",
      "65/65 - 0s - loss: 0.7052 - mean_squared_error: 0.7052 - val_loss: 0.7370 - val_mean_squared_error: 0.7370 - 78ms/epoch - 1ms/step\n",
      "Epoch 52/200\n",
      "65/65 - 0s - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 0.7178 - val_mean_squared_error: 0.7178 - 77ms/epoch - 1ms/step\n",
      "Epoch 53/200\n",
      "65/65 - 0s - loss: 0.7274 - mean_squared_error: 0.7274 - val_loss: 0.7279 - val_mean_squared_error: 0.7279 - 79ms/epoch - 1ms/step\n",
      "Epoch 54/200\n",
      "65/65 - 0s - loss: 0.7169 - mean_squared_error: 0.7169 - val_loss: 0.7439 - val_mean_squared_error: 0.7439 - 77ms/epoch - 1ms/step\n",
      "Epoch 55/200\n",
      "65/65 - 0s - loss: 0.7091 - mean_squared_error: 0.7091 - val_loss: 1.1720 - val_mean_squared_error: 1.1720 - 75ms/epoch - 1ms/step\n",
      "Epoch 56/200\n",
      "65/65 - 0s - loss: 0.7523 - mean_squared_error: 0.7523 - val_loss: 0.8362 - val_mean_squared_error: 0.8362 - 77ms/epoch - 1ms/step\n",
      "Epoch 57/200\n",
      "65/65 - 0s - loss: 0.7272 - mean_squared_error: 0.7272 - val_loss: 0.7197 - val_mean_squared_error: 0.7197 - 76ms/epoch - 1ms/step\n",
      "Epoch 58/200\n",
      "65/65 - 0s - loss: 0.7088 - mean_squared_error: 0.7088 - val_loss: 0.6856 - val_mean_squared_error: 0.6856 - 78ms/epoch - 1ms/step\n",
      "Epoch 59/200\n",
      "65/65 - 0s - loss: 0.7033 - mean_squared_error: 0.7033 - val_loss: 0.6705 - val_mean_squared_error: 0.6705 - 79ms/epoch - 1ms/step\n",
      "Epoch 60/200\n",
      "65/65 - 0s - loss: 0.7180 - mean_squared_error: 0.7180 - val_loss: 0.7148 - val_mean_squared_error: 0.7148 - 112ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "65/65 - 0s - loss: 0.7198 - mean_squared_error: 0.7198 - val_loss: 0.7594 - val_mean_squared_error: 0.7594 - 75ms/epoch - 1ms/step\n",
      "Epoch 62/200\n",
      "65/65 - 0s - loss: 0.6908 - mean_squared_error: 0.6908 - val_loss: 0.6876 - val_mean_squared_error: 0.6876 - 71ms/epoch - 1ms/step\n",
      "Epoch 63/200\n",
      "65/65 - 0s - loss: 0.7129 - mean_squared_error: 0.7129 - val_loss: 0.7776 - val_mean_squared_error: 0.7776 - 75ms/epoch - 1ms/step\n",
      "Epoch 64/200\n",
      "65/65 - 0s - loss: 0.7110 - mean_squared_error: 0.7110 - val_loss: 0.7322 - val_mean_squared_error: 0.7322 - 72ms/epoch - 1ms/step\n",
      "Epoch 65/200\n",
      "65/65 - 0s - loss: 0.7073 - mean_squared_error: 0.7073 - val_loss: 0.6793 - val_mean_squared_error: 0.6793 - 76ms/epoch - 1ms/step\n",
      "Epoch 66/200\n",
      "65/65 - 0s - loss: 0.6983 - mean_squared_error: 0.6983 - val_loss: 0.6654 - val_mean_squared_error: 0.6654 - 75ms/epoch - 1ms/step\n",
      "Epoch 67/200\n",
      "65/65 - 0s - loss: 0.6987 - mean_squared_error: 0.6987 - val_loss: 0.6840 - val_mean_squared_error: 0.6840 - 78ms/epoch - 1ms/step\n",
      "Epoch 68/200\n",
      "65/65 - 0s - loss: 0.7154 - mean_squared_error: 0.7154 - val_loss: 0.7486 - val_mean_squared_error: 0.7486 - 76ms/epoch - 1ms/step\n",
      "Epoch 69/200\n",
      "65/65 - 0s - loss: 0.7189 - mean_squared_error: 0.7189 - val_loss: 0.6766 - val_mean_squared_error: 0.6766 - 71ms/epoch - 1ms/step\n",
      "Epoch 70/200\n",
      "65/65 - 0s - loss: 0.6959 - mean_squared_error: 0.6959 - val_loss: 0.7158 - val_mean_squared_error: 0.7158 - 74ms/epoch - 1ms/step\n",
      "Epoch 71/200\n",
      "65/65 - 0s - loss: 0.6897 - mean_squared_error: 0.6897 - val_loss: 0.7437 - val_mean_squared_error: 0.7437 - 73ms/epoch - 1ms/step\n",
      "Epoch 72/200\n",
      "65/65 - 0s - loss: 0.7049 - mean_squared_error: 0.7049 - val_loss: 0.6835 - val_mean_squared_error: 0.6835 - 73ms/epoch - 1ms/step\n",
      "Epoch 73/200\n",
      "65/65 - 0s - loss: 0.6984 - mean_squared_error: 0.6984 - val_loss: 0.7061 - val_mean_squared_error: 0.7061 - 71ms/epoch - 1ms/step\n",
      "Epoch 74/200\n",
      "65/65 - 0s - loss: 0.6896 - mean_squared_error: 0.6896 - val_loss: 0.6644 - val_mean_squared_error: 0.6644 - 79ms/epoch - 1ms/step\n",
      "Epoch 75/200\n",
      "65/65 - 0s - loss: 0.6916 - mean_squared_error: 0.6916 - val_loss: 1.0176 - val_mean_squared_error: 1.0176 - 75ms/epoch - 1ms/step\n",
      "Epoch 76/200\n",
      "65/65 - 0s - loss: 0.6812 - mean_squared_error: 0.6812 - val_loss: 0.6830 - val_mean_squared_error: 0.6830 - 76ms/epoch - 1ms/step\n",
      "Epoch 77/200\n",
      "65/65 - 0s - loss: 0.7161 - mean_squared_error: 0.7161 - val_loss: 0.7597 - val_mean_squared_error: 0.7597 - 76ms/epoch - 1ms/step\n",
      "Epoch 78/200\n",
      "65/65 - 0s - loss: 0.6933 - mean_squared_error: 0.6933 - val_loss: 0.6739 - val_mean_squared_error: 0.6739 - 74ms/epoch - 1ms/step\n",
      "Epoch 79/200\n",
      "65/65 - 0s - loss: 0.6764 - mean_squared_error: 0.6764 - val_loss: 0.7200 - val_mean_squared_error: 0.7200 - 71ms/epoch - 1ms/step\n",
      "Epoch 80/200\n",
      "65/65 - 0s - loss: 0.6737 - mean_squared_error: 0.6737 - val_loss: 0.7286 - val_mean_squared_error: 0.7286 - 75ms/epoch - 1ms/step\n",
      "Epoch 81/200\n",
      "65/65 - 0s - loss: 0.6977 - mean_squared_error: 0.6977 - val_loss: 0.7157 - val_mean_squared_error: 0.7157 - 71ms/epoch - 1ms/step\n",
      "Epoch 82/200\n",
      "65/65 - 0s - loss: 0.6904 - mean_squared_error: 0.6904 - val_loss: 0.7058 - val_mean_squared_error: 0.7058 - 74ms/epoch - 1ms/step\n",
      "Epoch 83/200\n",
      "65/65 - 0s - loss: 0.6856 - mean_squared_error: 0.6856 - val_loss: 0.8100 - val_mean_squared_error: 0.8100 - 76ms/epoch - 1ms/step\n",
      "Epoch 84/200\n",
      "65/65 - 0s - loss: 0.6994 - mean_squared_error: 0.6994 - val_loss: 0.7175 - val_mean_squared_error: 0.7175 - 73ms/epoch - 1ms/step\n",
      "Epoch 85/200\n",
      "65/65 - 0s - loss: 0.7012 - mean_squared_error: 0.7012 - val_loss: 0.6868 - val_mean_squared_error: 0.6868 - 74ms/epoch - 1ms/step\n",
      "Epoch 86/200\n",
      "65/65 - 0s - loss: 0.7078 - mean_squared_error: 0.7078 - val_loss: 0.6958 - val_mean_squared_error: 0.6958 - 73ms/epoch - 1ms/step\n",
      "Epoch 87/200\n",
      "65/65 - 0s - loss: 0.6964 - mean_squared_error: 0.6964 - val_loss: 0.7291 - val_mean_squared_error: 0.7291 - 72ms/epoch - 1ms/step\n",
      "Epoch 88/200\n",
      "65/65 - 0s - loss: 0.6878 - mean_squared_error: 0.6878 - val_loss: 0.6632 - val_mean_squared_error: 0.6632 - 73ms/epoch - 1ms/step\n",
      "Epoch 89/200\n",
      "65/65 - 0s - loss: 0.6789 - mean_squared_error: 0.6789 - val_loss: 0.6878 - val_mean_squared_error: 0.6878 - 73ms/epoch - 1ms/step\n",
      "Epoch 90/200\n",
      "65/65 - 0s - loss: 0.6923 - mean_squared_error: 0.6923 - val_loss: 0.7205 - val_mean_squared_error: 0.7205 - 76ms/epoch - 1ms/step\n",
      "Epoch 91/200\n",
      "65/65 - 0s - loss: 0.6948 - mean_squared_error: 0.6948 - val_loss: 0.6809 - val_mean_squared_error: 0.6809 - 74ms/epoch - 1ms/step\n",
      "Epoch 92/200\n",
      "65/65 - 0s - loss: 0.6885 - mean_squared_error: 0.6885 - val_loss: 0.7611 - val_mean_squared_error: 0.7611 - 71ms/epoch - 1ms/step\n",
      "Epoch 93/200\n",
      "65/65 - 0s - loss: 0.7099 - mean_squared_error: 0.7099 - val_loss: 0.6985 - val_mean_squared_error: 0.6985 - 71ms/epoch - 1ms/step\n",
      "Epoch 94/200\n",
      "65/65 - 0s - loss: 0.6935 - mean_squared_error: 0.6935 - val_loss: 1.0772 - val_mean_squared_error: 1.0772 - 72ms/epoch - 1ms/step\n",
      "Epoch 95/200\n",
      "65/65 - 0s - loss: 0.6911 - mean_squared_error: 0.6911 - val_loss: 0.7311 - val_mean_squared_error: 0.7311 - 73ms/epoch - 1ms/step\n",
      "Epoch 96/200\n",
      "65/65 - 0s - loss: 0.7035 - mean_squared_error: 0.7035 - val_loss: 0.7340 - val_mean_squared_error: 0.7340 - 72ms/epoch - 1ms/step\n",
      "Epoch 97/200\n",
      "65/65 - 0s - loss: 0.6913 - mean_squared_error: 0.6913 - val_loss: 0.6914 - val_mean_squared_error: 0.6914 - 74ms/epoch - 1ms/step\n",
      "Epoch 98/200\n",
      "65/65 - 0s - loss: 0.6704 - mean_squared_error: 0.6704 - val_loss: 0.7094 - val_mean_squared_error: 0.7094 - 73ms/epoch - 1ms/step\n",
      "Epoch 99/200\n",
      "65/65 - 0s - loss: 0.6939 - mean_squared_error: 0.6939 - val_loss: 0.8027 - val_mean_squared_error: 0.8027 - 71ms/epoch - 1ms/step\n",
      "Epoch 100/200\n",
      "65/65 - 0s - loss: 0.6838 - mean_squared_error: 0.6838 - val_loss: 0.7925 - val_mean_squared_error: 0.7925 - 71ms/epoch - 1ms/step\n",
      "Epoch 101/200\n",
      "65/65 - 0s - loss: 0.6912 - mean_squared_error: 0.6912 - val_loss: 0.8156 - val_mean_squared_error: 0.8156 - 72ms/epoch - 1ms/step\n",
      "Epoch 102/200\n",
      "65/65 - 0s - loss: 0.6960 - mean_squared_error: 0.6960 - val_loss: 0.7252 - val_mean_squared_error: 0.7252 - 71ms/epoch - 1ms/step\n",
      "Epoch 103/200\n",
      "65/65 - 0s - loss: 0.6759 - mean_squared_error: 0.6759 - val_loss: 0.7461 - val_mean_squared_error: 0.7461 - 71ms/epoch - 1ms/step\n",
      "Epoch 104/200\n",
      "65/65 - 0s - loss: 0.6945 - mean_squared_error: 0.6945 - val_loss: 0.8758 - val_mean_squared_error: 0.8758 - 72ms/epoch - 1ms/step\n",
      "Epoch 105/200\n",
      "65/65 - 0s - loss: 0.6928 - mean_squared_error: 0.6928 - val_loss: 0.6991 - val_mean_squared_error: 0.6991 - 71ms/epoch - 1ms/step\n",
      "Epoch 106/200\n",
      "65/65 - 0s - loss: 0.6693 - mean_squared_error: 0.6693 - val_loss: 0.8623 - val_mean_squared_error: 0.8623 - 70ms/epoch - 1ms/step\n",
      "Epoch 107/200\n",
      "65/65 - 0s - loss: 0.6789 - mean_squared_error: 0.6789 - val_loss: 0.7249 - val_mean_squared_error: 0.7249 - 72ms/epoch - 1ms/step\n",
      "Epoch 108/200\n",
      "65/65 - 0s - loss: 0.6861 - mean_squared_error: 0.6861 - val_loss: 0.7713 - val_mean_squared_error: 0.7713 - 82ms/epoch - 1ms/step\n",
      "Epoch 109/200\n",
      "65/65 - 0s - loss: 0.6901 - mean_squared_error: 0.6901 - val_loss: 0.8244 - val_mean_squared_error: 0.8244 - 75ms/epoch - 1ms/step\n",
      "Epoch 110/200\n",
      "65/65 - 0s - loss: 0.6883 - mean_squared_error: 0.6883 - val_loss: 1.0962 - val_mean_squared_error: 1.0962 - 75ms/epoch - 1ms/step\n",
      "Epoch 111/200\n",
      "65/65 - 0s - loss: 0.6758 - mean_squared_error: 0.6758 - val_loss: 0.7500 - val_mean_squared_error: 0.7500 - 72ms/epoch - 1ms/step\n",
      "Epoch 112/200\n",
      "65/65 - 0s - loss: 0.6835 - mean_squared_error: 0.6835 - val_loss: 0.6615 - val_mean_squared_error: 0.6615 - 72ms/epoch - 1ms/step\n",
      "Epoch 113/200\n",
      "65/65 - 0s - loss: 0.6701 - mean_squared_error: 0.6701 - val_loss: 0.7658 - val_mean_squared_error: 0.7658 - 71ms/epoch - 1ms/step\n",
      "Epoch 114/200\n",
      "65/65 - 0s - loss: 0.6851 - mean_squared_error: 0.6851 - val_loss: 0.6833 - val_mean_squared_error: 0.6833 - 71ms/epoch - 1ms/step\n",
      "Epoch 115/200\n",
      "65/65 - 0s - loss: 0.6928 - mean_squared_error: 0.6928 - val_loss: 0.6653 - val_mean_squared_error: 0.6653 - 74ms/epoch - 1ms/step\n",
      "Epoch 116/200\n",
      "65/65 - 0s - loss: 0.6954 - mean_squared_error: 0.6954 - val_loss: 0.6919 - val_mean_squared_error: 0.6919 - 73ms/epoch - 1ms/step\n",
      "Epoch 117/200\n",
      "65/65 - 0s - loss: 0.6839 - mean_squared_error: 0.6839 - val_loss: 0.7417 - val_mean_squared_error: 0.7417 - 71ms/epoch - 1ms/step\n",
      "Epoch 118/200\n",
      "65/65 - 0s - loss: 0.6780 - mean_squared_error: 0.6780 - val_loss: 0.7418 - val_mean_squared_error: 0.7418 - 73ms/epoch - 1ms/step\n",
      "Epoch 119/200\n",
      "65/65 - 0s - loss: 0.6681 - mean_squared_error: 0.6681 - val_loss: 0.6828 - val_mean_squared_error: 0.6828 - 73ms/epoch - 1ms/step\n",
      "Epoch 120/200\n",
      "65/65 - 0s - loss: 0.6805 - mean_squared_error: 0.6805 - val_loss: 0.6794 - val_mean_squared_error: 0.6794 - 74ms/epoch - 1ms/step\n",
      "Epoch 121/200\n",
      "65/65 - 0s - loss: 0.6837 - mean_squared_error: 0.6837 - val_loss: 0.6612 - val_mean_squared_error: 0.6612 - 73ms/epoch - 1ms/step\n",
      "Epoch 122/200\n",
      "65/65 - 0s - loss: 0.6737 - mean_squared_error: 0.6737 - val_loss: 0.7008 - val_mean_squared_error: 0.7008 - 68ms/epoch - 1ms/step\n",
      "Epoch 123/200\n",
      "65/65 - 0s - loss: 0.6922 - mean_squared_error: 0.6922 - val_loss: 0.7943 - val_mean_squared_error: 0.7943 - 65ms/epoch - 999us/step\n",
      "Epoch 124/200\n",
      "65/65 - 0s - loss: 0.6923 - mean_squared_error: 0.6923 - val_loss: 0.7102 - val_mean_squared_error: 0.7102 - 72ms/epoch - 1ms/step\n",
      "Epoch 125/200\n",
      "65/65 - 0s - loss: 0.6743 - mean_squared_error: 0.6743 - val_loss: 0.7113 - val_mean_squared_error: 0.7113 - 74ms/epoch - 1ms/step\n",
      "Epoch 126/200\n",
      "65/65 - 0s - loss: 0.6684 - mean_squared_error: 0.6684 - val_loss: 0.7712 - val_mean_squared_error: 0.7712 - 74ms/epoch - 1ms/step\n",
      "Epoch 127/200\n",
      "65/65 - 0s - loss: 0.6832 - mean_squared_error: 0.6832 - val_loss: 0.7008 - val_mean_squared_error: 0.7008 - 75ms/epoch - 1ms/step\n",
      "Epoch 128/200\n",
      "65/65 - 0s - loss: 0.6700 - mean_squared_error: 0.6700 - val_loss: 0.7215 - val_mean_squared_error: 0.7215 - 71ms/epoch - 1ms/step\n",
      "Epoch 129/200\n",
      "65/65 - 0s - loss: 0.6859 - mean_squared_error: 0.6859 - val_loss: 0.8134 - val_mean_squared_error: 0.8134 - 72ms/epoch - 1ms/step\n",
      "Epoch 130/200\n",
      "65/65 - 0s - loss: 0.6891 - mean_squared_error: 0.6891 - val_loss: 0.7136 - val_mean_squared_error: 0.7136 - 77ms/epoch - 1ms/step\n",
      "Epoch 131/200\n",
      "65/65 - 0s - loss: 0.6765 - mean_squared_error: 0.6765 - val_loss: 0.7346 - val_mean_squared_error: 0.7346 - 77ms/epoch - 1ms/step\n",
      "Epoch 132/200\n",
      "65/65 - 0s - loss: 0.6579 - mean_squared_error: 0.6579 - val_loss: 0.7172 - val_mean_squared_error: 0.7172 - 73ms/epoch - 1ms/step\n",
      "Epoch 133/200\n",
      "65/65 - 0s - loss: 0.6897 - mean_squared_error: 0.6897 - val_loss: 0.9664 - val_mean_squared_error: 0.9664 - 72ms/epoch - 1ms/step\n",
      "Epoch 134/200\n",
      "65/65 - 0s - loss: 0.6830 - mean_squared_error: 0.6830 - val_loss: 0.6899 - val_mean_squared_error: 0.6899 - 71ms/epoch - 1ms/step\n",
      "Epoch 135/200\n",
      "65/65 - 0s - loss: 0.6781 - mean_squared_error: 0.6781 - val_loss: 0.7083 - val_mean_squared_error: 0.7083 - 103ms/epoch - 2ms/step\n",
      "Epoch 136/200\n",
      "65/65 - 0s - loss: 0.6603 - mean_squared_error: 0.6603 - val_loss: 0.7290 - val_mean_squared_error: 0.7290 - 75ms/epoch - 1ms/step\n",
      "Epoch 137/200\n",
      "65/65 - 0s - loss: 0.6825 - mean_squared_error: 0.6825 - val_loss: 0.8098 - val_mean_squared_error: 0.8098 - 73ms/epoch - 1ms/step\n",
      "Epoch 138/200\n",
      "65/65 - 0s - loss: 0.6998 - mean_squared_error: 0.6998 - val_loss: 0.9317 - val_mean_squared_error: 0.9317 - 72ms/epoch - 1ms/step\n",
      "Epoch 139/200\n",
      "65/65 - 0s - loss: 0.6824 - mean_squared_error: 0.6824 - val_loss: 0.8154 - val_mean_squared_error: 0.8154 - 72ms/epoch - 1ms/step\n",
      "Epoch 140/200\n",
      "65/65 - 0s - loss: 0.6847 - mean_squared_error: 0.6847 - val_loss: 0.6700 - val_mean_squared_error: 0.6700 - 74ms/epoch - 1ms/step\n",
      "Epoch 141/200\n",
      "65/65 - 0s - loss: 0.6656 - mean_squared_error: 0.6656 - val_loss: 0.7107 - val_mean_squared_error: 0.7107 - 72ms/epoch - 1ms/step\n",
      "Epoch 142/200\n",
      "65/65 - 0s - loss: 0.6627 - mean_squared_error: 0.6627 - val_loss: 0.7004 - val_mean_squared_error: 0.7004 - 73ms/epoch - 1ms/step\n",
      "Epoch 143/200\n",
      "65/65 - 0s - loss: 0.6660 - mean_squared_error: 0.6660 - val_loss: 0.7551 - val_mean_squared_error: 0.7551 - 75ms/epoch - 1ms/step\n",
      "Epoch 144/200\n",
      "65/65 - 0s - loss: 0.6713 - mean_squared_error: 0.6713 - val_loss: 0.7254 - val_mean_squared_error: 0.7254 - 73ms/epoch - 1ms/step\n",
      "Epoch 145/200\n",
      "65/65 - 0s - loss: 0.6755 - mean_squared_error: 0.6755 - val_loss: 0.6628 - val_mean_squared_error: 0.6628 - 73ms/epoch - 1ms/step\n",
      "Epoch 146/200\n",
      "65/65 - 0s - loss: 0.6693 - mean_squared_error: 0.6693 - val_loss: 0.7903 - val_mean_squared_error: 0.7903 - 74ms/epoch - 1ms/step\n",
      "Epoch 147/200\n",
      "65/65 - 0s - loss: 0.6777 - mean_squared_error: 0.6777 - val_loss: 0.6971 - val_mean_squared_error: 0.6971 - 77ms/epoch - 1ms/step\n",
      "Epoch 148/200\n",
      "65/65 - 0s - loss: 0.6709 - mean_squared_error: 0.6709 - val_loss: 0.6578 - val_mean_squared_error: 0.6578 - 84ms/epoch - 1ms/step\n",
      "Epoch 149/200\n",
      "65/65 - 0s - loss: 0.6796 - mean_squared_error: 0.6796 - val_loss: 0.8090 - val_mean_squared_error: 0.8090 - 75ms/epoch - 1ms/step\n",
      "Epoch 150/200\n",
      "65/65 - 0s - loss: 0.6850 - mean_squared_error: 0.6850 - val_loss: 0.6736 - val_mean_squared_error: 0.6736 - 73ms/epoch - 1ms/step\n",
      "Epoch 151/200\n",
      "65/65 - 0s - loss: 0.6683 - mean_squared_error: 0.6683 - val_loss: 0.6653 - val_mean_squared_error: 0.6653 - 72ms/epoch - 1ms/step\n",
      "Epoch 152/200\n",
      "65/65 - 0s - loss: 0.6790 - mean_squared_error: 0.6790 - val_loss: 0.8043 - val_mean_squared_error: 0.8043 - 74ms/epoch - 1ms/step\n",
      "Epoch 153/200\n",
      "65/65 - 0s - loss: 0.6687 - mean_squared_error: 0.6687 - val_loss: 0.6893 - val_mean_squared_error: 0.6893 - 72ms/epoch - 1ms/step\n",
      "Epoch 154/200\n",
      "65/65 - 0s - loss: 0.6912 - mean_squared_error: 0.6912 - val_loss: 0.7046 - val_mean_squared_error: 0.7046 - 73ms/epoch - 1ms/step\n",
      "Epoch 155/200\n",
      "65/65 - 0s - loss: 0.6742 - mean_squared_error: 0.6742 - val_loss: 0.6385 - val_mean_squared_error: 0.6385 - 74ms/epoch - 1ms/step\n",
      "Epoch 156/200\n",
      "65/65 - 0s - loss: 0.6697 - mean_squared_error: 0.6697 - val_loss: 0.8243 - val_mean_squared_error: 0.8243 - 74ms/epoch - 1ms/step\n",
      "Epoch 157/200\n",
      "65/65 - 0s - loss: 0.6622 - mean_squared_error: 0.6622 - val_loss: 0.6806 - val_mean_squared_error: 0.6806 - 71ms/epoch - 1ms/step\n",
      "Epoch 158/200\n",
      "65/65 - 0s - loss: 0.6572 - mean_squared_error: 0.6572 - val_loss: 0.6687 - val_mean_squared_error: 0.6687 - 73ms/epoch - 1ms/step\n",
      "Epoch 159/200\n",
      "65/65 - 0s - loss: 0.6761 - mean_squared_error: 0.6761 - val_loss: 0.7152 - val_mean_squared_error: 0.7152 - 77ms/epoch - 1ms/step\n",
      "Epoch 160/200\n",
      "65/65 - 0s - loss: 0.6818 - mean_squared_error: 0.6818 - val_loss: 0.7447 - val_mean_squared_error: 0.7447 - 75ms/epoch - 1ms/step\n",
      "Epoch 161/200\n",
      "65/65 - 0s - loss: 0.6701 - mean_squared_error: 0.6701 - val_loss: 0.6702 - val_mean_squared_error: 0.6702 - 74ms/epoch - 1ms/step\n",
      "Epoch 162/200\n",
      "65/65 - 0s - loss: 0.6743 - mean_squared_error: 0.6743 - val_loss: 0.8086 - val_mean_squared_error: 0.8086 - 73ms/epoch - 1ms/step\n",
      "Epoch 163/200\n",
      "65/65 - 0s - loss: 0.6747 - mean_squared_error: 0.6747 - val_loss: 0.7285 - val_mean_squared_error: 0.7285 - 72ms/epoch - 1ms/step\n",
      "Epoch 164/200\n",
      "65/65 - 0s - loss: 0.6761 - mean_squared_error: 0.6761 - val_loss: 0.7880 - val_mean_squared_error: 0.7880 - 72ms/epoch - 1ms/step\n",
      "Epoch 165/200\n",
      "65/65 - 0s - loss: 0.6610 - mean_squared_error: 0.6610 - val_loss: 0.7244 - val_mean_squared_error: 0.7244 - 71ms/epoch - 1ms/step\n",
      "Epoch 166/200\n",
      "65/65 - 0s - loss: 0.6553 - mean_squared_error: 0.6553 - val_loss: 0.7731 - val_mean_squared_error: 0.7731 - 71ms/epoch - 1ms/step\n",
      "Epoch 167/200\n",
      "65/65 - 0s - loss: 0.6623 - mean_squared_error: 0.6623 - val_loss: 0.6488 - val_mean_squared_error: 0.6488 - 73ms/epoch - 1ms/step\n",
      "Epoch 168/200\n",
      "65/65 - 0s - loss: 0.6744 - mean_squared_error: 0.6744 - val_loss: 0.7371 - val_mean_squared_error: 0.7371 - 74ms/epoch - 1ms/step\n",
      "Epoch 169/200\n",
      "65/65 - 0s - loss: 0.6686 - mean_squared_error: 0.6686 - val_loss: 0.8087 - val_mean_squared_error: 0.8087 - 73ms/epoch - 1ms/step\n",
      "Epoch 170/200\n",
      "65/65 - 0s - loss: 0.6715 - mean_squared_error: 0.6715 - val_loss: 0.6894 - val_mean_squared_error: 0.6894 - 75ms/epoch - 1ms/step\n",
      "Epoch 171/200\n",
      "65/65 - 0s - loss: 0.6828 - mean_squared_error: 0.6828 - val_loss: 0.6962 - val_mean_squared_error: 0.6962 - 71ms/epoch - 1ms/step\n",
      "Epoch 172/200\n",
      "65/65 - 0s - loss: 0.6638 - mean_squared_error: 0.6638 - val_loss: 0.7239 - val_mean_squared_error: 0.7239 - 74ms/epoch - 1ms/step\n",
      "Epoch 173/200\n",
      "65/65 - 0s - loss: 0.6757 - mean_squared_error: 0.6757 - val_loss: 0.7373 - val_mean_squared_error: 0.7373 - 73ms/epoch - 1ms/step\n",
      "Epoch 174/200\n",
      "65/65 - 0s - loss: 0.6682 - mean_squared_error: 0.6682 - val_loss: 0.6801 - val_mean_squared_error: 0.6801 - 74ms/epoch - 1ms/step\n",
      "Epoch 175/200\n",
      "65/65 - 0s - loss: 0.6655 - mean_squared_error: 0.6655 - val_loss: 0.7230 - val_mean_squared_error: 0.7230 - 76ms/epoch - 1ms/step\n",
      "Epoch 176/200\n",
      "65/65 - 0s - loss: 0.6525 - mean_squared_error: 0.6525 - val_loss: 0.6896 - val_mean_squared_error: 0.6896 - 73ms/epoch - 1ms/step\n",
      "Epoch 177/200\n",
      "65/65 - 0s - loss: 0.6569 - mean_squared_error: 0.6569 - val_loss: 0.7022 - val_mean_squared_error: 0.7022 - 72ms/epoch - 1ms/step\n",
      "Epoch 178/200\n",
      "65/65 - 0s - loss: 0.6608 - mean_squared_error: 0.6608 - val_loss: 0.6728 - val_mean_squared_error: 0.6728 - 72ms/epoch - 1ms/step\n",
      "Epoch 179/200\n",
      "65/65 - 0s - loss: 0.6690 - mean_squared_error: 0.6690 - val_loss: 0.7503 - val_mean_squared_error: 0.7503 - 73ms/epoch - 1ms/step\n",
      "Epoch 180/200\n",
      "65/65 - 0s - loss: 0.6652 - mean_squared_error: 0.6652 - val_loss: 0.7074 - val_mean_squared_error: 0.7074 - 74ms/epoch - 1ms/step\n",
      "Epoch 181/200\n",
      "65/65 - 0s - loss: 0.6661 - mean_squared_error: 0.6661 - val_loss: 0.7057 - val_mean_squared_error: 0.7057 - 72ms/epoch - 1ms/step\n",
      "Epoch 182/200\n",
      "65/65 - 0s - loss: 0.6690 - mean_squared_error: 0.6690 - val_loss: 0.7161 - val_mean_squared_error: 0.7161 - 74ms/epoch - 1ms/step\n",
      "Epoch 183/200\n",
      "65/65 - 0s - loss: 0.6706 - mean_squared_error: 0.6706 - val_loss: 0.6632 - val_mean_squared_error: 0.6632 - 71ms/epoch - 1ms/step\n",
      "Epoch 184/200\n",
      "65/65 - 0s - loss: 0.6518 - mean_squared_error: 0.6518 - val_loss: 0.6722 - val_mean_squared_error: 0.6722 - 92ms/epoch - 1ms/step\n",
      "Epoch 185/200\n",
      "65/65 - 0s - loss: 0.6628 - mean_squared_error: 0.6628 - val_loss: 0.7442 - val_mean_squared_error: 0.7442 - 81ms/epoch - 1ms/step\n",
      "Epoch 186/200\n",
      "65/65 - 0s - loss: 0.6549 - mean_squared_error: 0.6549 - val_loss: 0.7344 - val_mean_squared_error: 0.7344 - 73ms/epoch - 1ms/step\n",
      "Epoch 187/200\n",
      "65/65 - 0s - loss: 0.6899 - mean_squared_error: 0.6899 - val_loss: 0.8445 - val_mean_squared_error: 0.8445 - 73ms/epoch - 1ms/step\n",
      "Epoch 188/200\n",
      "65/65 - 0s - loss: 0.6580 - mean_squared_error: 0.6580 - val_loss: 0.7768 - val_mean_squared_error: 0.7768 - 71ms/epoch - 1ms/step\n",
      "Epoch 189/200\n",
      "65/65 - 0s - loss: 0.6647 - mean_squared_error: 0.6647 - val_loss: 0.7477 - val_mean_squared_error: 0.7477 - 71ms/epoch - 1ms/step\n",
      "Epoch 190/200\n",
      "65/65 - 0s - loss: 0.6497 - mean_squared_error: 0.6497 - val_loss: 0.9046 - val_mean_squared_error: 0.9046 - 73ms/epoch - 1ms/step\n",
      "Epoch 191/200\n",
      "65/65 - 0s - loss: 0.6521 - mean_squared_error: 0.6521 - val_loss: 0.6572 - val_mean_squared_error: 0.6572 - 77ms/epoch - 1ms/step\n",
      "Epoch 192/200\n",
      "65/65 - 0s - loss: 0.6649 - mean_squared_error: 0.6649 - val_loss: 0.8311 - val_mean_squared_error: 0.8311 - 79ms/epoch - 1ms/step\n",
      "Epoch 193/200\n",
      "65/65 - 0s - loss: 0.6649 - mean_squared_error: 0.6649 - val_loss: 0.6889 - val_mean_squared_error: 0.6889 - 70ms/epoch - 1ms/step\n",
      "Epoch 194/200\n",
      "65/65 - 0s - loss: 0.6724 - mean_squared_error: 0.6724 - val_loss: 0.6601 - val_mean_squared_error: 0.6601 - 70ms/epoch - 1ms/step\n",
      "Epoch 195/200\n",
      "65/65 - 0s - loss: 0.6593 - mean_squared_error: 0.6593 - val_loss: 0.6739 - val_mean_squared_error: 0.6739 - 72ms/epoch - 1ms/step\n",
      "Epoch 196/200\n",
      "65/65 - 0s - loss: 0.6637 - mean_squared_error: 0.6637 - val_loss: 0.7183 - val_mean_squared_error: 0.7183 - 68ms/epoch - 1ms/step\n",
      "Epoch 197/200\n",
      "65/65 - 0s - loss: 0.6743 - mean_squared_error: 0.6743 - val_loss: 0.7644 - val_mean_squared_error: 0.7644 - 73ms/epoch - 1ms/step\n",
      "Epoch 198/200\n",
      "65/65 - 0s - loss: 0.6697 - mean_squared_error: 0.6697 - val_loss: 0.7308 - val_mean_squared_error: 0.7308 - 69ms/epoch - 1ms/step\n",
      "Epoch 199/200\n",
      "65/65 - 0s - loss: 0.6702 - mean_squared_error: 0.6702 - val_loss: 0.6802 - val_mean_squared_error: 0.6802 - 72ms/epoch - 1ms/step\n",
      "Epoch 200/200\n",
      "65/65 - 0s - loss: 0.6561 - mean_squared_error: 0.6561 - val_loss: 0.7159 - val_mean_squared_error: 0.7159 - 70ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss='mean_squared_error', \n",
    "    metrics=['mean_squared_error']\n",
    ")\n",
    "batched_history = model.fit(x=train_samples, y=train_labels, batch_size=40, epochs=200, validation_split=0.2 ,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluando el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"606.08125pt\" height=\"392.514375pt\" viewBox=\"0 0 606.08125 392.514375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-08-06T14:37:36.490748</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 392.514375 \n",
       "L 606.08125 392.514375 \n",
       "L 606.08125 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 598.88125 354.958125 \n",
       "L 598.88125 22.318125 \n",
       "L 40.88125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"me44eff6f00\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"63.695777\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(60.514527 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"127.423507\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(121.061007 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"191.151236\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(184.788736 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"254.878966\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 75 -->\n",
       "      <g transform=\"translate(248.516466 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"318.606695\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(309.062945 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"382.334425\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 125 -->\n",
       "      <g transform=\"translate(372.790675 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"446.062155\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(436.518405 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"509.789884\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 175 -->\n",
       "      <g transform=\"translate(500.246134 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#me44eff6f00\" x=\"573.517614\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(563.973864 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Epochs -->\n",
       "     <g transform=\"translate(301.965625 383.234687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"306.201172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m00f8af3333\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.51875 358.757344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(27.51875 292.229344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"221.902125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(27.51875 225.701344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"155.374125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(27.51875 159.173344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"88.846125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(27.51875 92.645344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00f8af3333\" x=\"40.88125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(21.15625 26.117344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- mean_squared_error -->\n",
       "     <g transform=\"translate(14.798437 240.125625) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"158.935547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"220.214844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"283.59375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"333.59375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"385.693359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"449.169922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"512.548828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"573.828125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"612.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"674.214844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"737.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"787.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"849.214844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"888.578125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"927.441406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"988.623047\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 69.901274 -1 \n",
       "L 71.343105 60.21946 \n",
       "L 73.892214 116.090174 \n",
       "L 76.441323 142.862676 \n",
       "L 78.990432 173.57302 \n",
       "L 81.539541 199.660353 \n",
       "L 84.088651 240.363631 \n",
       "L 86.63776 222.533738 \n",
       "L 89.186869 253.039184 \n",
       "L 91.735978 261.271855 \n",
       "L 94.285087 267.245895 \n",
       "L 96.834197 265.86408 \n",
       "L 99.383306 267.461279 \n",
       "L 101.932415 273.040755 \n",
       "L 104.481524 269.365897 \n",
       "L 107.030633 273.784803 \n",
       "L 109.579742 275.836807 \n",
       "L 112.128852 272.637738 \n",
       "L 114.677961 277.474722 \n",
       "L 117.22707 277.814737 \n",
       "L 119.776179 276.063872 \n",
       "L 122.325288 268.824766 \n",
       "L 124.874398 279.067091 \n",
       "L 127.423507 275.327605 \n",
       "L 129.972616 278.451958 \n",
       "L 132.521725 274.282093 \n",
       "L 135.070834 277.186114 \n",
       "L 137.619943 271.496796 \n",
       "L 140.169053 277.00212 \n",
       "L 142.718162 272.060887 \n",
       "L 145.267271 276.848208 \n",
       "L 147.81638 276.123289 \n",
       "L 150.365489 277.22936 \n",
       "L 152.914599 275.879387 \n",
       "L 155.463708 272.723557 \n",
       "L 158.012817 277.708544 \n",
       "L 160.561926 274.301159 \n",
       "L 163.111035 280.482413 \n",
       "L 165.660144 321.471715 \n",
       "L 168.209254 321.90728 \n",
       "L 170.758363 322.645358 \n",
       "L 173.307472 322.147056 \n",
       "L 175.856581 322.393431 \n",
       "L 180.9548 322.621411 \n",
       "L 183.503909 322.940659 \n",
       "L 186.053018 322.497741 \n",
       "L 188.602127 322.781382 \n",
       "L 193.700345 322.945227 \n",
       "L 196.249455 322.691554 \n",
       "L 198.798564 323.265382 \n",
       "L 201.347673 323.196561 \n",
       "L 203.896782 322.943156 \n",
       "L 206.445891 322.571907 \n",
       "L 208.995001 322.473917 \n",
       "L 211.54411 323.062529 \n",
       "L 214.093219 322.96212 \n",
       "L 216.642328 322.234309 \n",
       "L 219.191437 323.027634 \n",
       "L 224.289656 323.193111 \n",
       "L 226.838765 322.763621 \n",
       "L 229.387874 323.155888 \n",
       "L 231.936983 323.146302 \n",
       "L 234.486092 323.438485 \n",
       "L 237.035202 323.022114 \n",
       "L 239.584311 323.180244 \n",
       "L 242.13342 323.51738 \n",
       "L 244.682529 323.415186 \n",
       "L 247.231638 323.06585 \n",
       "L 249.780747 323.539655 \n",
       "L 252.329857 323.643332 \n",
       "L 254.878966 323.208061 \n",
       "L 257.428075 323.324805 \n",
       "L 259.977184 321.879964 \n",
       "L 262.526293 323.505636 \n",
       "L 270.173621 323.191678 \n",
       "L 272.72273 323.346627 \n",
       "L 275.271839 323.271493 \n",
       "L 280.370058 323.356711 \n",
       "L 282.919167 323.312364 \n",
       "L 285.468276 324.122169 \n",
       "L 288.017385 323.5433 \n",
       "L 290.566494 323.891876 \n",
       "L 293.115604 323.455171 \n",
       "L 295.664713 323.694369 \n",
       "L 298.213822 324.045745 \n",
       "L 300.762931 323.931383 \n",
       "L 303.31204 325.126861 \n",
       "L 305.861149 326.006908 \n",
       "L 308.410259 328.401823 \n",
       "L 310.959368 329.689334 \n",
       "L 313.508477 330.42075 \n",
       "L 318.606695 330.880113 \n",
       "L 321.155805 331.182624 \n",
       "L 323.704914 330.871829 \n",
       "L 326.254023 331.25223 \n",
       "L 328.803132 331.07107 \n",
       "L 331.352241 331.556436 \n",
       "L 333.901351 330.953764 \n",
       "L 336.45046 331.008567 \n",
       "L 341.548678 331.437958 \n",
       "L 344.097787 329.968667 \n",
       "L 346.646896 328.693984 \n",
       "L 351.745115 330.609175 \n",
       "L 354.294224 330.337364 \n",
       "L 356.843333 330.5327 \n",
       "L 359.392442 331.247495 \n",
       "L 364.490661 331.072458 \n",
       "L 367.03977 331.541136 \n",
       "L 369.588879 331.524667 \n",
       "L 372.137988 331.681617 \n",
       "L 374.687097 331.957296 \n",
       "L 377.236207 331.80816 \n",
       "L 379.785316 332.057572 \n",
       "L 382.334425 331.868731 \n",
       "L 384.883534 331.951643 \n",
       "L 387.432643 332.167897 \n",
       "L 389.981753 332.574979 \n",
       "L 392.530862 331.637367 \n",
       "L 395.079971 332.321421 \n",
       "L 397.62908 331.982613 \n",
       "L 400.178189 332.623694 \n",
       "L 402.727298 331.862995 \n",
       "L 405.276408 332.405679 \n",
       "L 407.825517 331.985143 \n",
       "L 410.374626 332.518484 \n",
       "L 412.923735 332.044723 \n",
       "L 415.472844 331.24165 \n",
       "L 418.021954 332.467809 \n",
       "L 420.571063 332.02941 \n",
       "L 425.669281 332.648588 \n",
       "L 428.21839 332.256028 \n",
       "L 430.767499 332.741368 \n",
       "L 433.316609 332.331027 \n",
       "L 440.963936 332.35195 \n",
       "L 443.513045 332.106396 \n",
       "L 448.611264 332.657606 \n",
       "L 451.160373 332.428702 \n",
       "L 453.709482 332.466195 \n",
       "L 456.258591 332.005557 \n",
       "L 461.35681 332.404706 \n",
       "L 463.905919 332.459701 \n",
       "L 466.455028 332.678271 \n",
       "L 471.553246 332.175182 \n",
       "L 474.102356 332.662749 \n",
       "L 476.651465 332.460195 \n",
       "L 479.200574 332.475795 \n",
       "L 484.298792 332.901885 \n",
       "L 491.94612 332.836835 \n",
       "L 494.495229 333.03798 \n",
       "L 497.044338 332.861127 \n",
       "L 499.593447 332.474155 \n",
       "L 502.142557 332.77253 \n",
       "L 509.789884 332.558854 \n",
       "L 512.338993 332.957253 \n",
       "L 514.888102 332.786028 \n",
       "L 517.437212 332.444768 \n",
       "L 519.986321 332.594272 \n",
       "L 522.53543 333.151214 \n",
       "L 527.633648 332.902194 \n",
       "L 530.182758 332.451051 \n",
       "L 532.731867 333.360453 \n",
       "L 537.830085 332.708703 \n",
       "L 540.379194 332.638358 \n",
       "L 542.928303 333.092998 \n",
       "L 545.477413 332.954081 \n",
       "L 548.026522 333.457684 \n",
       "L 550.575631 333.010217 \n",
       "L 553.12474 332.435261 \n",
       "L 555.673849 332.712742 \n",
       "L 558.222959 332.768053 \n",
       "L 560.772068 333.2266 \n",
       "L 563.321177 333.507307 \n",
       "L 565.870286 332.833309 \n",
       "L 568.419395 332.741485 \n",
       "L 570.968504 332.922657 \n",
       "L 573.517614 332.719305 \n",
       "L 573.517614 332.719305 \n",
       "\" clip-path=\"url(#p5b8d3a645d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 68.488247 -1 \n",
       "L 68.793996 48.975647 \n",
       "L 69.207935 -1 \n",
       "M 78.022684 -1 \n",
       "L 78.990432 101.973017 \n",
       "L 81.539541 144.849283 \n",
       "L 84.088651 57.398173 \n",
       "L 86.63776 194.038716 \n",
       "L 89.186869 290.09466 \n",
       "L 91.735978 316.489281 \n",
       "L 94.285087 317.053654 \n",
       "L 96.834197 312.494647 \n",
       "L 99.383306 322.467352 \n",
       "L 101.932415 311.570583 \n",
       "L 104.481524 312.661216 \n",
       "L 107.030633 305.781738 \n",
       "L 109.579742 320.090427 \n",
       "L 112.128852 303.575188 \n",
       "L 114.677961 315.772388 \n",
       "L 117.22707 296.042655 \n",
       "L 119.776179 315.512323 \n",
       "L 122.325288 316.603833 \n",
       "L 124.874398 313.877588 \n",
       "L 127.423507 315.736819 \n",
       "L 129.972616 303.136839 \n",
       "L 132.521725 313.682472 \n",
       "L 135.070834 311.995782 \n",
       "L 137.619943 314.8896 \n",
       "L 140.169053 299.021134 \n",
       "L 142.718162 295.455621 \n",
       "L 145.267271 296.742346 \n",
       "L 147.81638 299.650907 \n",
       "L 150.365489 309.3082 \n",
       "L 152.914599 310.394963 \n",
       "L 155.463708 284.917585 \n",
       "L 158.012817 290.194869 \n",
       "L 160.561926 296.156501 \n",
       "L 163.111035 209.016598 \n",
       "L 165.660144 270.793425 \n",
       "L 168.209254 297.843468 \n",
       "L 170.758363 306.275297 \n",
       "L 173.307472 305.913044 \n",
       "L 175.856581 306.958131 \n",
       "L 178.40569 310.646586 \n",
       "L 180.9548 310.982383 \n",
       "L 183.503909 310.180777 \n",
       "L 186.053018 309.823104 \n",
       "L 188.602127 310.336276 \n",
       "L 193.700345 310.400372 \n",
       "L 196.249455 311.957758 \n",
       "L 198.798564 310.837151 \n",
       "L 201.347673 310.247158 \n",
       "L 203.896782 310.415393 \n",
       "L 206.445891 314.075572 \n",
       "L 208.995001 314.942407 \n",
       "L 211.54411 309.636145 \n",
       "L 214.093219 303.95375 \n",
       "L 216.642328 304.965449 \n",
       "L 219.191437 309.206869 \n",
       "L 221.740546 306.207984 \n",
       "L 224.289656 305.844058 \n",
       "L 226.838765 309.016519 \n",
       "L 229.387874 310.405329 \n",
       "L 231.936983 312.050826 \n",
       "L 234.486092 310.424727 \n",
       "L 237.035202 310.291804 \n",
       "L 239.584311 308.779417 \n",
       "L 242.13342 308.169177 \n",
       "L 244.682529 308.466077 \n",
       "L 247.231638 306.536623 \n",
       "L 249.780747 309.237918 \n",
       "L 252.329857 309.237723 \n",
       "L 254.878966 308.548315 \n",
       "L 257.428075 308.911147 \n",
       "L 259.977184 299.600269 \n",
       "L 262.526293 308.578987 \n",
       "L 265.075403 308.727102 \n",
       "L 267.624512 308.025067 \n",
       "L 270.173621 311.285924 \n",
       "L 272.72273 310.328742 \n",
       "L 275.271839 310.912402 \n",
       "L 277.820948 311.24844 \n",
       "L 280.370058 314.492305 \n",
       "L 282.919167 311.710168 \n",
       "L 285.468276 314.779656 \n",
       "L 288.017385 311.445134 \n",
       "L 290.566494 313.434632 \n",
       "L 293.115604 315.949268 \n",
       "L 295.664713 318.70195 \n",
       "L 298.213822 311.470223 \n",
       "L 300.762931 314.553372 \n",
       "L 303.31204 310.247713 \n",
       "L 305.861149 283.899261 \n",
       "L 308.410259 204.337833 \n",
       "L 310.959368 249.889317 \n",
       "L 313.508477 276.909267 \n",
       "L 316.057586 295.567833 \n",
       "L 318.606695 293.882369 \n",
       "L 321.155805 299.927754 \n",
       "L 323.704914 313.006141 \n",
       "L 326.254023 318.539076 \n",
       "L 328.803132 321.494123 \n",
       "L 331.352241 314.12456 \n",
       "L 333.901351 320.263742 \n",
       "L 336.45046 327.758748 \n",
       "L 338.999569 317.359067 \n",
       "L 341.548678 319.652348 \n",
       "L 344.097787 81.193517 \n",
       "L 346.646896 283.89102 \n",
       "L 349.196006 293.457118 \n",
       "L 351.745115 315.297015 \n",
       "L 354.294224 310.722194 \n",
       "L 356.843333 299.209112 \n",
       "L 359.392442 317.005078 \n",
       "L 361.941552 317.155322 \n",
       "L 364.490661 315.799206 \n",
       "L 367.03977 319.905621 \n",
       "L 369.588879 321.924186 \n",
       "L 372.137988 322.155497 \n",
       "L 374.687097 317.880788 \n",
       "L 379.785316 318.511683 \n",
       "L 382.334425 318.690177 \n",
       "L 384.883534 322.336661 \n",
       "L 387.432643 324.549441 \n",
       "L 389.981753 318.382575 \n",
       "L 392.530862 323.23103 \n",
       "L 395.079971 320.432187 \n",
       "L 397.62908 316.556181 \n",
       "L 400.178189 325.542019 \n",
       "L 402.727298 327.657942 \n",
       "L 405.276408 324.369532 \n",
       "L 407.825517 323.162423 \n",
       "L 410.374626 321.730533 \n",
       "L 412.923735 323.750669 \n",
       "L 415.472844 323.562068 \n",
       "L 418.021954 322.897736 \n",
       "L 420.571063 321.357869 \n",
       "L 423.120172 320.849944 \n",
       "L 425.669281 324.21965 \n",
       "L 428.21839 326.520725 \n",
       "L 430.767499 324.752587 \n",
       "L 433.316609 326.9557 \n",
       "L 438.414827 328.535833 \n",
       "L 440.963936 328.18455 \n",
       "L 443.513045 323.145812 \n",
       "L 446.062155 328.991392 \n",
       "L 448.611264 329.08871 \n",
       "L 451.160373 315.478296 \n",
       "L 453.709482 325.729389 \n",
       "L 456.258591 310.108425 \n",
       "L 458.8077 316.513192 \n",
       "L 461.35681 325.723918 \n",
       "L 463.905919 326.085551 \n",
       "L 466.455028 327.867921 \n",
       "L 469.004137 330.986445 \n",
       "L 471.553246 329.908669 \n",
       "L 474.102356 330.689707 \n",
       "L 476.651465 330.49032 \n",
       "L 479.200574 329.46071 \n",
       "L 481.749683 329.239906 \n",
       "L 484.298792 330.036033 \n",
       "L 486.847901 330.671651 \n",
       "L 489.397011 332.498483 \n",
       "L 491.94612 332.111502 \n",
       "L 494.495229 330.227198 \n",
       "L 497.044338 332.693125 \n",
       "L 499.593447 330.525404 \n",
       "L 502.142557 330.113058 \n",
       "L 504.691666 331.825925 \n",
       "L 507.240775 328.097766 \n",
       "L 509.789884 329.648607 \n",
       "L 512.338993 328.502871 \n",
       "L 514.888102 330.496824 \n",
       "L 517.437212 328.755247 \n",
       "L 519.986321 329.90073 \n",
       "L 522.53543 328.399946 \n",
       "L 525.084539 328.903707 \n",
       "L 527.633648 322.522867 \n",
       "L 530.182758 328.843701 \n",
       "L 532.731867 329.895252 \n",
       "L 535.280976 328.061626 \n",
       "L 537.830085 329.541509 \n",
       "L 540.379194 329.368178 \n",
       "L 542.928303 328.676386 \n",
       "L 545.477413 329.348395 \n",
       "L 548.026522 324.798597 \n",
       "L 550.575631 329.227151 \n",
       "L 553.12474 328.705337 \n",
       "L 555.673849 327.830105 \n",
       "L 558.222959 325.398016 \n",
       "L 560.772068 324.264213 \n",
       "L 563.321177 322.424337 \n",
       "L 565.870286 328.110527 \n",
       "L 568.419395 328.944255 \n",
       "L 570.968504 328.618154 \n",
       "L 573.517614 329.018255 \n",
       "L 573.517614 329.018255 \n",
       "\" clip-path=\"url(#p5b8d3a645d)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 40.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 598.88125 354.958125 \n",
       "L 598.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 598.88125 354.958125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.88125 22.318125 \n",
       "L 598.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Training and validation mean_squared_error -->\n",
       "    <g transform=\"translate(187.875625 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"87.447266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"148.726562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"176.509766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"239.888672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"267.671875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"331.050781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"394.527344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"426.314453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"487.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"550.972656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"614.449219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"646.236328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"705.416016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"766.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"794.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"822.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"885.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"947.017578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"986.226562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1014.009766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1075.191406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1138.570312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1170.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1267.769531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"1329.292969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1390.572266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-5f\" x=\"1453.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1503.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-71\" x=\"1556.050781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"1619.527344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"1682.90625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"1744.185547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1783.048828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"1844.572266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-5f\" x=\"1908.048828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1958.048828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2019.572266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2058.935547\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"2097.798828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2158.980469\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 428.63125 60.230625 \n",
       "L 591.88125 60.230625 \n",
       "Q 593.88125 60.230625 593.88125 58.230625 \n",
       "L 593.88125 29.318125 \n",
       "Q 593.88125 27.318125 591.88125 27.318125 \n",
       "L 428.63125 27.318125 \n",
       "Q 426.63125 27.318125 426.63125 29.318125 \n",
       "L 426.63125 58.230625 \n",
       "Q 426.63125 60.230625 428.63125 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 430.63125 35.416563 \n",
       "L 440.63125 35.416563 \n",
       "L 450.63125 35.416563 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- train_mean_squared_error -->\n",
       "     <g transform=\"translate(458.63125 38.916563) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"441.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"502.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"566.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"616.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"668.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"731.933594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"795.3125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"856.591797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"895.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"956.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"1020.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1070.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1131.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1171.341797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"1210.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1271.386719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 430.63125 50.372813 \n",
       "L 440.63125 50.372813 \n",
       "L 450.63125 50.372813 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- val_mean_squared_error -->\n",
       "     <g transform=\"translate(458.63125 53.872813) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"295.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"357.177734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"418.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"481.835938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"531.835938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" x=\"583.935547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"647.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"710.791016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"772.070312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"810.933594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"872.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"935.933594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"985.933594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1047.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1086.820312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"1125.683594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1186.865234\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5b8d3a645d\">\n",
       "   <rect x=\"40.88125\" y=\"22.318125\" width=\"558\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "def plot_metric(history, metric):\n",
    "\t\tplt.figure(figsize=(10, 6))\n",
    "\t\ttrain_metrics = history.history[metric]\n",
    "\t\tval_metrics = history.history['val_'+metric]\n",
    "\t\tepochs = range(1, len(train_metrics) + 1)\n",
    "\t\tplt.plot(epochs, train_metrics)\n",
    "\t\tplt.plot(epochs, val_metrics)\n",
    "\t\tplt.title('Training and validation '+ metric)\n",
    "\t\tplt.xlabel(\"Epochs\")\n",
    "\t\tplt.ylabel(metric)\n",
    "\t\tplt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "\t\tplt.ylim([0,10])\n",
    "\t\tplt.show()\n",
    "\n",
    "plot_metric(batched_history, 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "test loss, test mse: [0.7180644869804382, 0.7180644869804382]\n",
      "26/26 [==============================] - 0s 391us/step\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
    "print(\"test loss, test mse:\", results)\n",
    "predictions = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_127 (Dense)           (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_87 (Ba  (None, 16)                64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 128)               2176      \n",
      "                                                                 \n",
      " batch_normalization_88 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_89 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_90 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13633 (53.25 KB)\n",
      "Trainable params: 13153 (51.38 KB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/61 [==============================] - 1s 3ms/step - loss: 6.0899 - mean_absolute_error: 6.0899 - val_loss: 5.3977 - val_mean_absolute_error: 5.3977\n",
      "Epoch 2/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 5.1912 - mean_absolute_error: 5.1912 - val_loss: 4.7882 - val_mean_absolute_error: 4.7882\n",
      "Epoch 3/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 3.7776 - mean_absolute_error: 3.7776 - val_loss: 2.6719 - val_mean_absolute_error: 2.6719\n",
      "Epoch 4/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 2.0152 - mean_absolute_error: 2.0152 - val_loss: 1.2241 - val_mean_absolute_error: 1.2241\n",
      "Epoch 5/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.9416 - mean_absolute_error: 0.9416 - val_loss: 0.8772 - val_mean_absolute_error: 0.8772\n",
      "Epoch 6/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.8234 - mean_absolute_error: 0.8234 - val_loss: 0.7863 - val_mean_absolute_error: 0.7863\n",
      "Epoch 7/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.8043 - mean_absolute_error: 0.8043 - val_loss: 0.8019 - val_mean_absolute_error: 0.8019\n",
      "Epoch 8/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7993 - mean_absolute_error: 0.7993 - val_loss: 0.7631 - val_mean_absolute_error: 0.7631\n",
      "Epoch 9/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7899 - mean_absolute_error: 0.7899 - val_loss: 0.7714 - val_mean_absolute_error: 0.7714\n",
      "Epoch 10/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7875 - mean_absolute_error: 0.7875 - val_loss: 0.7626 - val_mean_absolute_error: 0.7626\n",
      "Epoch 11/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7829 - mean_absolute_error: 0.7829 - val_loss: 0.7466 - val_mean_absolute_error: 0.7466\n",
      "Epoch 12/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7839 - mean_absolute_error: 0.7839 - val_loss: 0.7719 - val_mean_absolute_error: 0.7719\n",
      "Epoch 13/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7810 - mean_absolute_error: 0.7810 - val_loss: 0.7558 - val_mean_absolute_error: 0.7558\n",
      "Epoch 14/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7791 - mean_absolute_error: 0.7791 - val_loss: 0.7416 - val_mean_absolute_error: 0.7416\n",
      "Epoch 15/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7726 - mean_absolute_error: 0.7726 - val_loss: 0.7558 - val_mean_absolute_error: 0.7558\n",
      "Epoch 16/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7610 - mean_absolute_error: 0.7610 - val_loss: 0.7735 - val_mean_absolute_error: 0.7735\n",
      "Epoch 17/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7591 - mean_absolute_error: 0.7591 - val_loss: 0.7441 - val_mean_absolute_error: 0.7441\n",
      "Epoch 18/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7663 - mean_absolute_error: 0.7663 - val_loss: 0.7591 - val_mean_absolute_error: 0.7591\n",
      "Epoch 19/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7788 - mean_absolute_error: 0.7788 - val_loss: 0.7513 - val_mean_absolute_error: 0.7513\n",
      "Epoch 20/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7697 - mean_absolute_error: 0.7697 - val_loss: 0.7605 - val_mean_absolute_error: 0.7605\n",
      "Epoch 21/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7651 - mean_absolute_error: 0.7651 - val_loss: 0.7368 - val_mean_absolute_error: 0.7368\n",
      "Epoch 22/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7646 - mean_absolute_error: 0.7646 - val_loss: 0.7719 - val_mean_absolute_error: 0.7719\n",
      "Epoch 23/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7705 - mean_absolute_error: 0.7705 - val_loss: 0.7465 - val_mean_absolute_error: 0.7465\n",
      "Epoch 24/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7641 - mean_absolute_error: 0.7641 - val_loss: 0.7423 - val_mean_absolute_error: 0.7423\n",
      "Epoch 25/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7624 - mean_absolute_error: 0.7624 - val_loss: 0.7308 - val_mean_absolute_error: 0.7308\n",
      "Epoch 26/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7604 - mean_absolute_error: 0.7604 - val_loss: 0.7468 - val_mean_absolute_error: 0.7468\n",
      "Epoch 27/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7659 - mean_absolute_error: 0.7659 - val_loss: 0.7403 - val_mean_absolute_error: 0.7403\n",
      "Epoch 28/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7598 - mean_absolute_error: 0.7598 - val_loss: 0.7692 - val_mean_absolute_error: 0.7692\n",
      "Epoch 29/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7569 - mean_absolute_error: 0.7569 - val_loss: 0.7496 - val_mean_absolute_error: 0.7496\n",
      "Epoch 30/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7561 - mean_absolute_error: 0.7561 - val_loss: 0.7534 - val_mean_absolute_error: 0.7534\n",
      "Epoch 31/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7628 - mean_absolute_error: 0.7628 - val_loss: 0.7578 - val_mean_absolute_error: 0.7578\n",
      "Epoch 32/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7540 - mean_absolute_error: 0.7540 - val_loss: 0.7345 - val_mean_absolute_error: 0.7345\n",
      "Epoch 33/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7655 - mean_absolute_error: 0.7655 - val_loss: 0.7371 - val_mean_absolute_error: 0.7371\n",
      "Epoch 34/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7513 - mean_absolute_error: 0.7513 - val_loss: 0.7394 - val_mean_absolute_error: 0.7394\n",
      "Epoch 35/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7574 - mean_absolute_error: 0.7574 - val_loss: 0.7334 - val_mean_absolute_error: 0.7334\n",
      "Epoch 36/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7595 - mean_absolute_error: 0.7595 - val_loss: 0.7261 - val_mean_absolute_error: 0.7261\n",
      "Epoch 37/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7568 - mean_absolute_error: 0.7568 - val_loss: 0.7427 - val_mean_absolute_error: 0.7427\n",
      "Epoch 38/200\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.7570 - mean_absolute_error: 0.7570 - val_loss: 0.7321 - val_mean_absolute_error: 0.7321\n",
      "Epoch 39/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7568 - mean_absolute_error: 0.7568 - val_loss: 0.7502 - val_mean_absolute_error: 0.7502\n",
      "Epoch 40/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7517 - mean_absolute_error: 0.7517 - val_loss: 0.7221 - val_mean_absolute_error: 0.7221\n",
      "Epoch 41/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7465 - mean_absolute_error: 0.7465 - val_loss: 0.7125 - val_mean_absolute_error: 0.7125\n",
      "Epoch 42/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7424 - mean_absolute_error: 0.7424 - val_loss: 0.7446 - val_mean_absolute_error: 0.7446\n",
      "Epoch 43/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7548 - mean_absolute_error: 0.7548 - val_loss: 0.7276 - val_mean_absolute_error: 0.7276\n",
      "Epoch 44/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7356 - mean_absolute_error: 0.7356 - val_loss: 0.7675 - val_mean_absolute_error: 0.7675\n",
      "Epoch 45/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7374 - mean_absolute_error: 0.7374 - val_loss: 0.8736 - val_mean_absolute_error: 0.8736\n",
      "Epoch 46/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7233 - mean_absolute_error: 0.7233 - val_loss: 0.7708 - val_mean_absolute_error: 0.7708\n",
      "Epoch 47/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7230 - mean_absolute_error: 0.7230 - val_loss: 0.7954 - val_mean_absolute_error: 0.7954\n",
      "Epoch 48/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7124 - mean_absolute_error: 0.7124 - val_loss: 0.8015 - val_mean_absolute_error: 0.8015\n",
      "Epoch 49/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6956 - mean_absolute_error: 0.6956 - val_loss: 0.7666 - val_mean_absolute_error: 0.7666\n",
      "Epoch 50/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6820 - mean_absolute_error: 0.6820 - val_loss: 0.7693 - val_mean_absolute_error: 0.7693\n",
      "Epoch 51/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6817 - mean_absolute_error: 0.6817 - val_loss: 0.7168 - val_mean_absolute_error: 0.7168\n",
      "Epoch 52/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6620 - mean_absolute_error: 0.6620 - val_loss: 0.7066 - val_mean_absolute_error: 0.7066\n",
      "Epoch 53/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6588 - mean_absolute_error: 0.6588 - val_loss: 0.7309 - val_mean_absolute_error: 0.7309\n",
      "Epoch 54/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6506 - mean_absolute_error: 0.6506 - val_loss: 0.7620 - val_mean_absolute_error: 0.7620\n",
      "Epoch 55/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6578 - mean_absolute_error: 0.6578 - val_loss: 0.6717 - val_mean_absolute_error: 0.6717\n",
      "Epoch 56/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6539 - mean_absolute_error: 0.6539 - val_loss: 0.7669 - val_mean_absolute_error: 0.7669\n",
      "Epoch 57/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6588 - mean_absolute_error: 0.6588 - val_loss: 0.7186 - val_mean_absolute_error: 0.7186\n",
      "Epoch 58/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6531 - mean_absolute_error: 0.6531 - val_loss: 0.6692 - val_mean_absolute_error: 0.6692\n",
      "Epoch 59/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6443 - mean_absolute_error: 0.6443 - val_loss: 0.6870 - val_mean_absolute_error: 0.6870\n",
      "Epoch 60/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6576 - mean_absolute_error: 0.6576 - val_loss: 0.6936 - val_mean_absolute_error: 0.6936\n",
      "Epoch 61/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6474 - mean_absolute_error: 0.6474 - val_loss: 0.7121 - val_mean_absolute_error: 0.7121\n",
      "Epoch 62/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6481 - mean_absolute_error: 0.6481 - val_loss: 0.6590 - val_mean_absolute_error: 0.6590\n",
      "Epoch 63/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6452 - mean_absolute_error: 0.6452 - val_loss: 0.7100 - val_mean_absolute_error: 0.7100\n",
      "Epoch 64/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6515 - mean_absolute_error: 0.6515 - val_loss: 0.6896 - val_mean_absolute_error: 0.6896\n",
      "Epoch 65/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6383 - mean_absolute_error: 0.6383 - val_loss: 0.6273 - val_mean_absolute_error: 0.6273\n",
      "Epoch 66/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6312 - mean_absolute_error: 0.6312 - val_loss: 0.6610 - val_mean_absolute_error: 0.6610\n",
      "Epoch 67/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6343 - mean_absolute_error: 0.6343 - val_loss: 0.7384 - val_mean_absolute_error: 0.7384\n",
      "Epoch 68/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6409 - mean_absolute_error: 0.6409 - val_loss: 0.6562 - val_mean_absolute_error: 0.6562\n",
      "Epoch 69/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6414 - mean_absolute_error: 0.6414 - val_loss: 0.7416 - val_mean_absolute_error: 0.7416\n",
      "Epoch 70/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6343 - mean_absolute_error: 0.6343 - val_loss: 0.6866 - val_mean_absolute_error: 0.6866\n",
      "Epoch 71/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6385 - mean_absolute_error: 0.6385 - val_loss: 0.6181 - val_mean_absolute_error: 0.6181\n",
      "Epoch 72/200\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.6472 - mean_absolute_error: 0.6472 - val_loss: 0.6912 - val_mean_absolute_error: 0.6912\n",
      "Epoch 73/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6291 - mean_absolute_error: 0.6291 - val_loss: 0.6493 - val_mean_absolute_error: 0.6493\n",
      "Epoch 74/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6304 - mean_absolute_error: 0.6304 - val_loss: 0.6176 - val_mean_absolute_error: 0.6176\n",
      "Epoch 75/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6393 - mean_absolute_error: 0.6393 - val_loss: 0.6153 - val_mean_absolute_error: 0.6153\n",
      "Epoch 76/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6332 - mean_absolute_error: 0.6332 - val_loss: 0.7056 - val_mean_absolute_error: 0.7056\n",
      "Epoch 77/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6295 - mean_absolute_error: 0.6295 - val_loss: 0.6785 - val_mean_absolute_error: 0.6785\n",
      "Epoch 78/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6316 - mean_absolute_error: 0.6316 - val_loss: 0.6314 - val_mean_absolute_error: 0.6314\n",
      "Epoch 79/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6333 - mean_absolute_error: 0.6333 - val_loss: 0.6179 - val_mean_absolute_error: 0.6179\n",
      "Epoch 80/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6276 - mean_absolute_error: 0.6276 - val_loss: 0.6536 - val_mean_absolute_error: 0.6536\n",
      "Epoch 81/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6204 - mean_absolute_error: 0.6204 - val_loss: 0.6555 - val_mean_absolute_error: 0.6555\n",
      "Epoch 82/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6321 - mean_absolute_error: 0.6321 - val_loss: 0.6665 - val_mean_absolute_error: 0.6665\n",
      "Epoch 83/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6248 - mean_absolute_error: 0.6248 - val_loss: 0.6238 - val_mean_absolute_error: 0.6238\n",
      "Epoch 84/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6229 - mean_absolute_error: 0.6229 - val_loss: 0.8161 - val_mean_absolute_error: 0.8161\n",
      "Epoch 85/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6271 - mean_absolute_error: 0.6271 - val_loss: 0.6173 - val_mean_absolute_error: 0.6173\n",
      "Epoch 86/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6325 - mean_absolute_error: 0.6325 - val_loss: 0.6190 - val_mean_absolute_error: 0.6190\n",
      "Epoch 87/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6256 - mean_absolute_error: 0.6256 - val_loss: 0.6027 - val_mean_absolute_error: 0.6027\n",
      "Epoch 88/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6220 - mean_absolute_error: 0.6220 - val_loss: 0.6628 - val_mean_absolute_error: 0.6628\n",
      "Epoch 89/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6306 - mean_absolute_error: 0.6306 - val_loss: 0.6376 - val_mean_absolute_error: 0.6376\n",
      "Epoch 90/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6305 - mean_absolute_error: 0.6305 - val_loss: 0.6980 - val_mean_absolute_error: 0.6980\n",
      "Epoch 91/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6118 - mean_absolute_error: 0.6118 - val_loss: 0.6311 - val_mean_absolute_error: 0.6311\n",
      "Epoch 92/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6284 - mean_absolute_error: 0.6284 - val_loss: 0.6544 - val_mean_absolute_error: 0.6544\n",
      "Epoch 93/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6209 - mean_absolute_error: 0.6209 - val_loss: 0.6477 - val_mean_absolute_error: 0.6477\n",
      "Epoch 94/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6307 - mean_absolute_error: 0.6307 - val_loss: 0.6950 - val_mean_absolute_error: 0.6950\n",
      "Epoch 95/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6230 - mean_absolute_error: 0.6230 - val_loss: 0.6723 - val_mean_absolute_error: 0.6723\n",
      "Epoch 96/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6241 - mean_absolute_error: 0.6241 - val_loss: 0.6285 - val_mean_absolute_error: 0.6285\n",
      "Epoch 97/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6205 - mean_absolute_error: 0.6205 - val_loss: 0.6354 - val_mean_absolute_error: 0.6354\n",
      "Epoch 98/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6179 - mean_absolute_error: 0.6179 - val_loss: 0.6726 - val_mean_absolute_error: 0.6726\n",
      "Epoch 99/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6201 - mean_absolute_error: 0.6201 - val_loss: 0.6356 - val_mean_absolute_error: 0.6356\n",
      "Epoch 100/200\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.6233 - mean_absolute_error: 0.6233 - val_loss: 0.6306 - val_mean_absolute_error: 0.6306\n",
      "Epoch 101/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6246 - mean_absolute_error: 0.6246 - val_loss: 0.6320 - val_mean_absolute_error: 0.6320\n",
      "Epoch 102/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6137 - mean_absolute_error: 0.6137 - val_loss: 0.6195 - val_mean_absolute_error: 0.6195\n",
      "Epoch 103/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6204 - mean_absolute_error: 0.6204 - val_loss: 0.6173 - val_mean_absolute_error: 0.6173\n",
      "Epoch 104/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6214 - mean_absolute_error: 0.6214 - val_loss: 0.6655 - val_mean_absolute_error: 0.6655\n",
      "Epoch 105/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6131 - mean_absolute_error: 0.6131 - val_loss: 0.7063 - val_mean_absolute_error: 0.7063\n",
      "Epoch 106/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6300 - mean_absolute_error: 0.6300 - val_loss: 0.6225 - val_mean_absolute_error: 0.6225\n",
      "Epoch 107/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6229 - mean_absolute_error: 0.6229 - val_loss: 0.6371 - val_mean_absolute_error: 0.6371\n",
      "Epoch 108/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6160 - mean_absolute_error: 0.6160 - val_loss: 0.6510 - val_mean_absolute_error: 0.6510\n",
      "Epoch 109/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6118 - mean_absolute_error: 0.6118 - val_loss: 0.6772 - val_mean_absolute_error: 0.6772\n",
      "Epoch 110/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6148 - mean_absolute_error: 0.6148 - val_loss: 0.6253 - val_mean_absolute_error: 0.6253\n",
      "Epoch 111/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6117 - mean_absolute_error: 0.6117 - val_loss: 0.6230 - val_mean_absolute_error: 0.6230\n",
      "Epoch 112/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6223 - mean_absolute_error: 0.6223 - val_loss: 0.6125 - val_mean_absolute_error: 0.6125\n",
      "Epoch 113/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6161 - mean_absolute_error: 0.6161 - val_loss: 0.6379 - val_mean_absolute_error: 0.6379\n",
      "Epoch 114/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6185 - mean_absolute_error: 0.6185 - val_loss: 0.6833 - val_mean_absolute_error: 0.6833\n",
      "Epoch 115/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6160 - mean_absolute_error: 0.6160 - val_loss: 0.6587 - val_mean_absolute_error: 0.6587\n",
      "Epoch 116/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6211 - mean_absolute_error: 0.6211 - val_loss: 0.6453 - val_mean_absolute_error: 0.6453\n",
      "Epoch 117/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6230 - mean_absolute_error: 0.6230 - val_loss: 0.6511 - val_mean_absolute_error: 0.6511\n",
      "Epoch 118/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6241 - mean_absolute_error: 0.6241 - val_loss: 0.6210 - val_mean_absolute_error: 0.6210\n",
      "Epoch 119/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6175 - mean_absolute_error: 0.6175 - val_loss: 0.6750 - val_mean_absolute_error: 0.6750\n",
      "Epoch 120/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6055 - mean_absolute_error: 0.6055 - val_loss: 0.7465 - val_mean_absolute_error: 0.7465\n",
      "Epoch 121/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6177 - mean_absolute_error: 0.6177 - val_loss: 0.6195 - val_mean_absolute_error: 0.6195\n",
      "Epoch 122/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6195 - mean_absolute_error: 0.6195 - val_loss: 0.6351 - val_mean_absolute_error: 0.6351\n",
      "Epoch 123/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6084 - mean_absolute_error: 0.6084 - val_loss: 0.6200 - val_mean_absolute_error: 0.6200\n",
      "Epoch 124/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6119 - mean_absolute_error: 0.6119 - val_loss: 0.6132 - val_mean_absolute_error: 0.6132\n",
      "Epoch 125/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6141 - mean_absolute_error: 0.6141 - val_loss: 0.6290 - val_mean_absolute_error: 0.6290\n",
      "Epoch 126/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6180 - mean_absolute_error: 0.6180 - val_loss: 0.6626 - val_mean_absolute_error: 0.6626\n",
      "Epoch 127/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6138 - mean_absolute_error: 0.6138 - val_loss: 0.6233 - val_mean_absolute_error: 0.6233\n",
      "Epoch 128/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6076 - mean_absolute_error: 0.6076 - val_loss: 0.6294 - val_mean_absolute_error: 0.6294\n",
      "Epoch 129/200\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.6130 - mean_absolute_error: 0.6130 - val_loss: 0.6585 - val_mean_absolute_error: 0.6585\n",
      "Epoch 130/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6115 - mean_absolute_error: 0.6115 - val_loss: 0.6129 - val_mean_absolute_error: 0.6129\n",
      "Epoch 131/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6089 - mean_absolute_error: 0.6089 - val_loss: 0.6223 - val_mean_absolute_error: 0.6223\n",
      "Epoch 132/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6112 - mean_absolute_error: 0.6112 - val_loss: 0.6128 - val_mean_absolute_error: 0.6128\n",
      "Epoch 133/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6139 - mean_absolute_error: 0.6139 - val_loss: 0.6133 - val_mean_absolute_error: 0.6133\n",
      "Epoch 134/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6141 - mean_absolute_error: 0.6141 - val_loss: 0.6648 - val_mean_absolute_error: 0.6648\n",
      "Epoch 135/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6164 - mean_absolute_error: 0.6164 - val_loss: 0.6848 - val_mean_absolute_error: 0.6848\n",
      "Epoch 136/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6146 - mean_absolute_error: 0.6146 - val_loss: 0.6751 - val_mean_absolute_error: 0.6751\n",
      "Epoch 137/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6120 - mean_absolute_error: 0.6120 - val_loss: 0.7012 - val_mean_absolute_error: 0.7012\n",
      "Epoch 138/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6263 - mean_absolute_error: 0.6263 - val_loss: 0.6203 - val_mean_absolute_error: 0.6203\n",
      "Epoch 139/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6257 - mean_absolute_error: 0.6257 - val_loss: 0.6608 - val_mean_absolute_error: 0.6608\n",
      "Epoch 140/200\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6203 - mean_absolute_error: 0.6203 - val_loss: 0.6228 - val_mean_absolute_error: 0.6228\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(7,), activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(32, activation='tanh'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(1) \n",
    "])\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='mean_absolute_error', \n",
    "    patience=20, \n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam', loss='mean_absolute_error', \n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "batched_history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    validation_split=0.25, \n",
    "    batch_size=40, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"606.08125pt\" height=\"392.514375pt\" viewBox=\"0 0 606.08125 392.514375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-08-06T14:42:38.140607</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 392.514375 \n",
       "L 606.08125 392.514375 \n",
       "L 606.08125 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 598.88125 354.958125 \n",
       "L 598.88125 22.318125 \n",
       "L 40.88125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m313f0e5e6b\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"62.595442\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(59.414192 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"135.584324\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(129.221824 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"208.573206\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(202.210706 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"281.562087\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(275.199587 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"354.550969\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(348.188469 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"427.53985\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(417.9961 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"500.528732\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 120 -->\n",
       "      <g transform=\"translate(490.984982 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m313f0e5e6b\" x=\"573.517614\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 140 -->\n",
       "      <g transform=\"translate(563.973864 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- Epochs -->\n",
       "     <g transform=\"translate(301.965625 383.234687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"306.201172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <defs>\n",
       "       <path id=\"m561d5286ce\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.51875 358.757344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(27.51875 292.229344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"221.902125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(27.51875 225.701344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"155.374125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(27.51875 159.173344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"88.846125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(27.51875 92.645344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m561d5286ce\" x=\"40.88125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(21.15625 26.117344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- mean_absolute_error -->\n",
       "     <g transform=\"translate(14.798437 241.417031) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"158.935547\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"220.214844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"283.59375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"333.59375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"394.873047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"458.349609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"510.449219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"571.630859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"599.414062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"662.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"702.001953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"763.525391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"813.525391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"875.048828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"914.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"953.275391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1014.457031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 66.244886 152.382898 \n",
       "L 69.89433 182.27804 \n",
       "L 73.543775 229.298411 \n",
       "L 77.193219 287.924738 \n",
       "L 80.842663 323.63515 \n",
       "L 84.492107 327.569413 \n",
       "L 88.141551 328.20545 \n",
       "L 91.790995 328.369319 \n",
       "L 95.440439 328.683218 \n",
       "L 117.337103 329.256775 \n",
       "L 120.986548 329.64439 \n",
       "L 124.635992 329.707971 \n",
       "L 128.285436 329.468807 \n",
       "L 131.93488 329.050797 \n",
       "L 135.584324 329.354507 \n",
       "L 139.233768 329.509401 \n",
       "L 142.883212 329.526057 \n",
       "L 146.532656 329.327249 \n",
       "L 150.1821 329.541166 \n",
       "L 157.480988 329.663174 \n",
       "L 161.130432 329.482175 \n",
       "L 168.429321 329.779643 \n",
       "L 172.078765 329.808864 \n",
       "L 175.728209 329.582913 \n",
       "L 179.377653 329.878651 \n",
       "L 183.027097 329.49349 \n",
       "L 186.676541 329.965413 \n",
       "L 190.325985 329.764468 \n",
       "L 193.975429 329.695601 \n",
       "L 201.274317 329.776396 \n",
       "L 204.923761 329.782665 \n",
       "L 215.872094 330.262856 \n",
       "L 219.521538 329.850743 \n",
       "L 223.170982 330.489686 \n",
       "L 226.820426 330.429182 \n",
       "L 230.46987 330.899863 \n",
       "L 234.119314 330.907754 \n",
       "L 237.768758 331.262286 \n",
       "L 245.067646 332.271059 \n",
       "L 248.71709 332.282643 \n",
       "L 252.366534 332.936824 \n",
       "L 256.015979 333.044448 \n",
       "L 259.665423 333.317296 \n",
       "L 263.314867 333.077819 \n",
       "L 266.964311 333.206206 \n",
       "L 270.613755 333.044908 \n",
       "L 277.912643 333.526339 \n",
       "L 281.562087 333.085026 \n",
       "L 285.211531 333.424627 \n",
       "L 288.860975 333.400749 \n",
       "L 292.510419 333.496265 \n",
       "L 296.159863 333.287773 \n",
       "L 299.809308 333.727322 \n",
       "L 303.458752 333.960532 \n",
       "L 307.108196 333.859032 \n",
       "L 310.75764 333.640083 \n",
       "L 314.407084 333.6237 \n",
       "L 318.056528 333.858373 \n",
       "L 321.705972 333.720148 \n",
       "L 325.355416 333.430408 \n",
       "L 329.00486 334.030616 \n",
       "L 332.654304 333.988783 \n",
       "L 336.303748 333.693767 \n",
       "L 343.602637 334.019762 \n",
       "L 350.901525 333.891901 \n",
       "L 358.200413 334.321607 \n",
       "L 361.849857 333.933317 \n",
       "L 365.499301 334.173155 \n",
       "L 369.148745 334.238021 \n",
       "L 376.447633 333.917872 \n",
       "L 380.097077 334.149256 \n",
       "L 383.746521 334.268969 \n",
       "L 387.395966 333.983041 \n",
       "L 391.04541 333.985486 \n",
       "L 394.694854 334.606819 \n",
       "L 398.344298 334.053845 \n",
       "L 401.993742 334.304054 \n",
       "L 405.643186 333.97837 \n",
       "L 409.29263 334.233702 \n",
       "L 412.942074 334.198532 \n",
       "L 420.240962 334.403883 \n",
       "L 431.189294 334.18019 \n",
       "L 434.838739 334.542568 \n",
       "L 438.488183 334.319713 \n",
       "L 442.137627 334.286581 \n",
       "L 445.787071 334.563231 \n",
       "L 449.436515 334.0029 \n",
       "L 460.384847 334.606507 \n",
       "L 464.034291 334.506497 \n",
       "L 467.683735 334.611321 \n",
       "L 471.333179 334.256817 \n",
       "L 474.982623 334.463355 \n",
       "L 478.632068 334.385406 \n",
       "L 482.281512 334.469034 \n",
       "L 489.5804 334.233661 \n",
       "L 493.229844 334.198232 \n",
       "L 496.879288 334.418759 \n",
       "L 500.528732 334.816026 \n",
       "L 504.178176 334.410876 \n",
       "L 507.82762 334.352638 \n",
       "L 511.477064 334.721501 \n",
       "L 522.425397 334.399838 \n",
       "L 529.724285 334.74569 \n",
       "L 533.373729 334.566427 \n",
       "L 544.322061 334.628166 \n",
       "L 555.270393 334.455306 \n",
       "L 562.569281 334.598993 \n",
       "L 566.218725 334.124254 \n",
       "L 569.86817 334.144648 \n",
       "L 573.517614 334.324387 \n",
       "L 573.517614 334.324387 \n",
       "\" clip-path=\"url(#p6575afa977)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 66.244886 175.410449 \n",
       "L 69.89433 195.681841 \n",
       "L 73.543775 266.080423 \n",
       "L 77.193219 314.239623 \n",
       "L 80.842663 325.778742 \n",
       "L 84.492107 328.801311 \n",
       "L 88.141551 328.283871 \n",
       "L 91.790995 329.575873 \n",
       "L 95.440439 329.297101 \n",
       "L 99.089883 329.589954 \n",
       "L 102.739327 330.122044 \n",
       "L 106.388771 329.281524 \n",
       "L 113.687659 330.289595 \n",
       "L 117.337103 329.818238 \n",
       "L 120.986548 329.228718 \n",
       "L 124.635992 330.206596 \n",
       "L 128.285436 329.707832 \n",
       "L 131.93488 329.966137 \n",
       "L 135.584324 329.660216 \n",
       "L 139.233768 330.449178 \n",
       "L 142.883212 329.283299 \n",
       "L 146.532656 330.125537 \n",
       "L 150.1821 330.266368 \n",
       "L 153.831544 330.649372 \n",
       "L 157.480988 330.116165 \n",
       "L 161.130432 330.331221 \n",
       "L 164.779877 329.370147 \n",
       "L 168.429321 330.023756 \n",
       "L 175.728209 329.750103 \n",
       "L 179.377653 330.526546 \n",
       "L 186.676541 330.363866 \n",
       "L 193.975429 330.804955 \n",
       "L 197.624873 330.251868 \n",
       "L 201.274317 330.60425 \n",
       "L 204.923761 330.003529 \n",
       "L 208.573206 330.939602 \n",
       "L 212.22265 331.258616 \n",
       "L 215.872094 330.188408 \n",
       "L 219.521538 330.754613 \n",
       "L 223.170982 329.429221 \n",
       "L 226.820426 325.900088 \n",
       "L 230.46987 329.318682 \n",
       "L 234.119314 328.499608 \n",
       "L 237.768758 328.295466 \n",
       "L 241.418202 329.456975 \n",
       "L 245.067646 329.368061 \n",
       "L 248.71709 331.114687 \n",
       "L 252.366534 331.455315 \n",
       "L 256.015979 330.646812 \n",
       "L 259.665423 329.611725 \n",
       "L 263.314867 332.615987 \n",
       "L 266.964311 329.446351 \n",
       "L 274.263199 332.697061 \n",
       "L 277.912643 332.107243 \n",
       "L 281.562087 331.887787 \n",
       "L 285.211531 331.270437 \n",
       "L 288.860975 333.038169 \n",
       "L 292.510419 331.341185 \n",
       "L 296.159863 332.018151 \n",
       "L 299.809308 334.092765 \n",
       "L 303.458752 332.970151 \n",
       "L 307.108196 330.396042 \n",
       "L 310.75764 333.131787 \n",
       "L 314.407084 330.289216 \n",
       "L 318.056528 332.120475 \n",
       "L 321.705972 334.397056 \n",
       "L 325.355416 331.965088 \n",
       "L 329.00486 333.359864 \n",
       "L 332.654304 334.413316 \n",
       "L 336.303748 334.489812 \n",
       "L 339.953192 331.486336 \n",
       "L 343.602637 332.388872 \n",
       "L 347.252081 333.955775 \n",
       "L 350.901525 334.405518 \n",
       "L 354.550969 333.215962 \n",
       "L 358.200413 333.153494 \n",
       "L 361.849857 332.786225 \n",
       "L 365.499301 334.209262 \n",
       "L 369.148745 327.812245 \n",
       "L 372.798189 334.423852 \n",
       "L 376.447633 334.366503 \n",
       "L 380.097077 334.910874 \n",
       "L 383.746521 332.911578 \n",
       "L 387.395966 333.749456 \n",
       "L 391.04541 331.73943 \n",
       "L 394.694854 333.965199 \n",
       "L 398.344298 333.190104 \n",
       "L 401.993742 333.414138 \n",
       "L 405.643186 331.83862 \n",
       "L 409.29263 332.595482 \n",
       "L 412.942074 334.051636 \n",
       "L 416.591518 333.823811 \n",
       "L 420.240962 332.585109 \n",
       "L 423.890406 333.814905 \n",
       "L 427.53985 333.981508 \n",
       "L 431.189294 333.935508 \n",
       "L 434.838739 334.35009 \n",
       "L 438.488183 334.424623 \n",
       "L 442.137627 332.822252 \n",
       "L 445.787071 331.463499 \n",
       "L 449.436515 334.251594 \n",
       "L 456.735403 333.304321 \n",
       "L 460.384847 332.430098 \n",
       "L 464.034291 334.15679 \n",
       "L 467.683735 334.235772 \n",
       "L 471.333179 334.583919 \n",
       "L 474.982623 333.738334 \n",
       "L 478.632068 332.229908 \n",
       "L 482.281512 333.046702 \n",
       "L 485.930956 333.494215 \n",
       "L 489.5804 333.300958 \n",
       "L 493.229844 334.301721 \n",
       "L 496.879288 332.505928 \n",
       "L 500.528732 330.128049 \n",
       "L 504.178176 334.352184 \n",
       "L 507.82762 333.831899 \n",
       "L 511.477064 334.333372 \n",
       "L 515.126508 334.5606 \n",
       "L 518.775952 334.036425 \n",
       "L 522.425397 332.916436 \n",
       "L 526.074841 334.22418 \n",
       "L 529.724285 334.021408 \n",
       "L 533.373729 333.053917 \n",
       "L 537.023173 334.569445 \n",
       "L 540.672617 334.258022 \n",
       "L 544.322061 334.57534 \n",
       "L 547.971505 334.556115 \n",
       "L 551.620949 332.844317 \n",
       "L 555.270393 332.177771 \n",
       "L 558.919837 332.50206 \n",
       "L 562.569281 331.63409 \n",
       "L 566.218725 334.324139 \n",
       "L 569.86817 332.978559 \n",
       "L 573.517614 334.241223 \n",
       "L 573.517614 334.241223 \n",
       "\" clip-path=\"url(#p6575afa977)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 40.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 598.88125 354.958125 \n",
       "L 598.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.88125 354.958125 \n",
       "L 598.88125 354.958125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.88125 22.318125 \n",
       "L 598.88125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- Training and validation mean_absolute_error -->\n",
       "    <g transform=\"translate(186.325938 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"87.447266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"148.726562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"176.509766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"239.888672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"267.671875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"331.050781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"394.527344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"426.314453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"487.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"550.972656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"614.449219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"646.236328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"705.416016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"766.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"794.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"822.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"885.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"947.017578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"986.226562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1014.009766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1075.191406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1138.570312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\" x=\"1170.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1267.769531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"1329.292969\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1390.572266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-5f\" x=\"1453.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"1503.951172\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" x=\"1565.230469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1628.707031\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1680.806641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"1741.988281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"1769.771484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"1833.150391\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1872.359375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-5f\" x=\"1933.882812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"1983.882812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2045.40625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2084.769531\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"2123.632812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"2184.814453\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 426.048437 60.230625 \n",
       "L 591.88125 60.230625 \n",
       "Q 593.88125 60.230625 593.88125 58.230625 \n",
       "L 593.88125 29.318125 \n",
       "Q 593.88125 27.318125 591.88125 27.318125 \n",
       "L 426.048437 27.318125 \n",
       "Q 424.048437 27.318125 424.048437 29.318125 \n",
       "L 424.048437 58.230625 \n",
       "Q 424.048437 60.230625 426.048437 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 428.048437 35.416563 \n",
       "L 438.048437 35.416563 \n",
       "L 448.048437 35.416563 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- train_mean_absolute_error -->\n",
       "     <g transform=\"translate(456.048437 38.916563) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"441.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"502.978516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"566.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"616.357422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"677.636719\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"741.113281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"793.212891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"854.394531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"882.177734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"945.556641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"984.765625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"1046.289062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1096.289062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1157.8125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1197.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"1236.039062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1297.220703\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 428.048437 50.372813 \n",
       "L 438.048437 50.372813 \n",
       "L 448.048437 50.372813 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- val_mean_absolute_error -->\n",
       "     <g transform=\"translate(456.048437 53.872813) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"295.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"357.177734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"418.457031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"481.835938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"531.835938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"593.115234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"656.591797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"708.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"769.873047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"797.65625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"861.035156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"900.244141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"961.767578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1011.767578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1073.291016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1112.654297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"1151.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"1212.699219\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6575afa977\">\n",
       "   <rect x=\"40.88125\" y=\"22.318125\" width=\"558\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(batched_history, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_162 (Dense)           (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_111 (B  (None, 16)                64        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 128)               2176      \n",
      "                                                                 \n",
      " batch_normalization_112 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_113 (B  (None, 64)                256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19841 (77.50 KB)\n",
      "Trainable params: 19425 (75.88 KB)\n",
      "Non-trainable params: 416 (1.62 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 4.3512 - mean_absolute_error: 4.8277 WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 1s 3ms/step - loss: 3.9212 - mean_absolute_error: 4.3893 - val_loss: 4.2470 - val_mean_absolute_error: 4.7310\n",
      "Epoch 2/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 1.3694 - mean_absolute_error: 1.7796WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 1.2784 - mean_absolute_error: 1.6865 - val_loss: 0.5316 - val_mean_absolute_error: 0.9139\n",
      "Epoch 3/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.5908 - mean_absolute_error: 0.9812WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.5856 - mean_absolute_error: 0.9757 - val_loss: 0.4327 - val_mean_absolute_error: 0.8011\n",
      "Epoch 4/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.5377 - mean_absolute_error: 0.9188WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.5454 - mean_absolute_error: 0.9280 - val_loss: 0.4146 - val_mean_absolute_error: 0.7805\n",
      "Epoch 5/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.4650 - mean_absolute_error: 0.8393WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4677 - mean_absolute_error: 0.8427 - val_loss: 0.4229 - val_mean_absolute_error: 0.7903\n",
      "Epoch 6/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.4776 - mean_absolute_error: 0.8531WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4777 - mean_absolute_error: 0.8532 - val_loss: 0.4023 - val_mean_absolute_error: 0.7608\n",
      "Epoch 7/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.4585 - mean_absolute_error: 0.8329WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4625 - mean_absolute_error: 0.8377 - val_loss: 0.3911 - val_mean_absolute_error: 0.7509\n",
      "Epoch 8/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4469 - mean_absolute_error: 0.8185WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4453 - mean_absolute_error: 0.8161 - val_loss: 0.4126 - val_mean_absolute_error: 0.7774\n",
      "Epoch 9/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4489 - mean_absolute_error: 0.8228WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4503 - mean_absolute_error: 0.8249 - val_loss: 0.4028 - val_mean_absolute_error: 0.7670\n",
      "Epoch 10/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4242 - mean_absolute_error: 0.7924WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4249 - mean_absolute_error: 0.7926 - val_loss: 0.4197 - val_mean_absolute_error: 0.7826\n",
      "Epoch 11/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.4190 - mean_absolute_error: 0.7894WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4226 - mean_absolute_error: 0.7932 - val_loss: 0.3875 - val_mean_absolute_error: 0.7443\n",
      "Epoch 12/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.4425 - mean_absolute_error: 0.8164WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4440 - mean_absolute_error: 0.8192 - val_loss: 0.3848 - val_mean_absolute_error: 0.7434\n",
      "Epoch 13/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4330 - mean_absolute_error: 0.8043WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4398 - mean_absolute_error: 0.8132 - val_loss: 0.3843 - val_mean_absolute_error: 0.7429\n",
      "Epoch 14/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.4462 - mean_absolute_error: 0.8175WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4425 - mean_absolute_error: 0.8135 - val_loss: 0.3947 - val_mean_absolute_error: 0.7530\n",
      "Epoch 15/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4264 - mean_absolute_error: 0.7954WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4252 - mean_absolute_error: 0.7936 - val_loss: 0.3891 - val_mean_absolute_error: 0.7517\n",
      "Epoch 16/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.4228 - mean_absolute_error: 0.7905WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4228 - mean_absolute_error: 0.7912 - val_loss: 0.3795 - val_mean_absolute_error: 0.7434\n",
      "Epoch 17/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4344 - mean_absolute_error: 0.8080WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4283 - mean_absolute_error: 0.8004 - val_loss: 0.3802 - val_mean_absolute_error: 0.7363\n",
      "Epoch 18/200\n",
      "30/61 [=============>................] - ETA: 0s - loss: 0.4153 - mean_absolute_error: 0.7850WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.4138 - mean_absolute_error: 0.7832 - val_loss: 0.3892 - val_mean_absolute_error: 0.7510\n",
      "Epoch 19/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.4392 - mean_absolute_error: 0.8100WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4333 - mean_absolute_error: 0.8033 - val_loss: 0.3870 - val_mean_absolute_error: 0.7420\n",
      "Epoch 20/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4326 - mean_absolute_error: 0.8039WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4283 - mean_absolute_error: 0.7992 - val_loss: 0.3916 - val_mean_absolute_error: 0.7500\n",
      "Epoch 21/200\n",
      "44/61 [====================>.........] - ETA: 0s - loss: 0.3978 - mean_absolute_error: 0.7604WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4130 - mean_absolute_error: 0.7767 - val_loss: 0.3966 - val_mean_absolute_error: 0.7543\n",
      "Epoch 22/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4267 - mean_absolute_error: 0.7968WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4255 - mean_absolute_error: 0.7943 - val_loss: 0.3956 - val_mean_absolute_error: 0.7550\n",
      "Epoch 23/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4218 - mean_absolute_error: 0.7916WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4230 - mean_absolute_error: 0.7936 - val_loss: 0.3846 - val_mean_absolute_error: 0.7387\n",
      "Epoch 24/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.4263 - mean_absolute_error: 0.8004WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4322 - mean_absolute_error: 0.8062 - val_loss: 0.3871 - val_mean_absolute_error: 0.7435\n",
      "Epoch 25/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4116 - mean_absolute_error: 0.7846WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4132 - mean_absolute_error: 0.7869 - val_loss: 0.3812 - val_mean_absolute_error: 0.7438\n",
      "Epoch 26/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4289 - mean_absolute_error: 0.8023WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4210 - mean_absolute_error: 0.7930 - val_loss: 0.3735 - val_mean_absolute_error: 0.7333\n",
      "Epoch 27/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.4131 - mean_absolute_error: 0.7811WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4217 - mean_absolute_error: 0.7907 - val_loss: 0.4068 - val_mean_absolute_error: 0.7780\n",
      "Epoch 28/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.4180 - mean_absolute_error: 0.7896WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4172 - mean_absolute_error: 0.7890 - val_loss: 0.4129 - val_mean_absolute_error: 0.7709\n",
      "Epoch 29/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4192 - mean_absolute_error: 0.7831WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4193 - mean_absolute_error: 0.7828 - val_loss: 0.4621 - val_mean_absolute_error: 0.8251\n",
      "Epoch 30/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4219 - mean_absolute_error: 0.7887WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4194 - mean_absolute_error: 0.7859 - val_loss: 0.4393 - val_mean_absolute_error: 0.8034\n",
      "Epoch 31/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4153 - mean_absolute_error: 0.7809WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4133 - mean_absolute_error: 0.7786 - val_loss: 0.3805 - val_mean_absolute_error: 0.7424\n",
      "Epoch 32/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4171 - mean_absolute_error: 0.7833WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4139 - mean_absolute_error: 0.7794 - val_loss: 0.3816 - val_mean_absolute_error: 0.7409\n",
      "Epoch 33/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.4086 - mean_absolute_error: 0.7699WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4084 - mean_absolute_error: 0.7700 - val_loss: 0.3763 - val_mean_absolute_error: 0.7353\n",
      "Epoch 34/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4165 - mean_absolute_error: 0.7873WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4139 - mean_absolute_error: 0.7840 - val_loss: 0.3823 - val_mean_absolute_error: 0.7396\n",
      "Epoch 35/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3995 - mean_absolute_error: 0.7629WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4027 - mean_absolute_error: 0.7685 - val_loss: 0.3712 - val_mean_absolute_error: 0.7283\n",
      "Epoch 36/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4055 - mean_absolute_error: 0.7738WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4073 - mean_absolute_error: 0.7751 - val_loss: 0.4059 - val_mean_absolute_error: 0.7646\n",
      "Epoch 37/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4024 - mean_absolute_error: 0.7655WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4080 - mean_absolute_error: 0.7726 - val_loss: 0.3769 - val_mean_absolute_error: 0.7373\n",
      "Epoch 38/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4099 - mean_absolute_error: 0.7763WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4101 - mean_absolute_error: 0.7754 - val_loss: 0.3786 - val_mean_absolute_error: 0.7357\n",
      "Epoch 39/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.4018 - mean_absolute_error: 0.7677WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4002 - mean_absolute_error: 0.7656 - val_loss: 0.3825 - val_mean_absolute_error: 0.7406\n",
      "Epoch 40/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4046 - mean_absolute_error: 0.7660WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4032 - mean_absolute_error: 0.7651 - val_loss: 0.3759 - val_mean_absolute_error: 0.7335\n",
      "Epoch 41/200\n",
      "45/61 [=====================>........] - ETA: 0s - loss: 0.4073 - mean_absolute_error: 0.7693WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4027 - mean_absolute_error: 0.7660 - val_loss: 0.3694 - val_mean_absolute_error: 0.7291\n",
      "Epoch 42/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3941 - mean_absolute_error: 0.7558WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3972 - mean_absolute_error: 0.7597 - val_loss: 0.3899 - val_mean_absolute_error: 0.7432\n",
      "Epoch 43/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4031 - mean_absolute_error: 0.7685WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4022 - mean_absolute_error: 0.7682 - val_loss: 0.3814 - val_mean_absolute_error: 0.7364\n",
      "Epoch 44/200\n",
      "38/61 [=================>............] - ETA: 0s - loss: 0.4183 - mean_absolute_error: 0.7851WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.4059 - mean_absolute_error: 0.7700 - val_loss: 0.3842 - val_mean_absolute_error: 0.7365\n",
      "Epoch 45/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3950 - mean_absolute_error: 0.7585WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4036 - mean_absolute_error: 0.7685 - val_loss: 0.3754 - val_mean_absolute_error: 0.7290\n",
      "Epoch 46/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3926 - mean_absolute_error: 0.7524WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3973 - mean_absolute_error: 0.7581 - val_loss: 0.3724 - val_mean_absolute_error: 0.7294\n",
      "Epoch 47/200\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 0.4126 - mean_absolute_error: 0.7792WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4107 - mean_absolute_error: 0.7769 - val_loss: 0.3977 - val_mean_absolute_error: 0.7607\n",
      "Epoch 48/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4075 - mean_absolute_error: 0.7717WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4116 - mean_absolute_error: 0.7767 - val_loss: 0.3717 - val_mean_absolute_error: 0.7289\n",
      "Epoch 49/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3922 - mean_absolute_error: 0.7569WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3963 - mean_absolute_error: 0.7612 - val_loss: 0.3698 - val_mean_absolute_error: 0.7300\n",
      "Epoch 50/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3982 - mean_absolute_error: 0.7637WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3976 - mean_absolute_error: 0.7624 - val_loss: 0.3769 - val_mean_absolute_error: 0.7323\n",
      "Epoch 51/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.4031 - mean_absolute_error: 0.7667WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4004 - mean_absolute_error: 0.7634 - val_loss: 0.3751 - val_mean_absolute_error: 0.7311\n",
      "Epoch 52/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3956 - mean_absolute_error: 0.7602WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3904 - mean_absolute_error: 0.7532 - val_loss: 0.3926 - val_mean_absolute_error: 0.7504\n",
      "Epoch 53/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3986 - mean_absolute_error: 0.7641WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4053 - mean_absolute_error: 0.7715 - val_loss: 0.3713 - val_mean_absolute_error: 0.7288\n",
      "Epoch 54/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3909 - mean_absolute_error: 0.7545WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3939 - mean_absolute_error: 0.7571 - val_loss: 0.3795 - val_mean_absolute_error: 0.7368\n",
      "Epoch 55/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3982 - mean_absolute_error: 0.7628WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4031 - mean_absolute_error: 0.7668 - val_loss: 0.3969 - val_mean_absolute_error: 0.7472\n",
      "Epoch 56/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3978 - mean_absolute_error: 0.7638WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3985 - mean_absolute_error: 0.7640 - val_loss: 0.3862 - val_mean_absolute_error: 0.7395\n",
      "Epoch 57/200\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 0.3940 - mean_absolute_error: 0.7546WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4043 - mean_absolute_error: 0.7648 - val_loss: 0.3676 - val_mean_absolute_error: 0.7214\n",
      "Epoch 58/200\n",
      "39/61 [==================>...........] - ETA: 0s - loss: 0.3825 - mean_absolute_error: 0.7379WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.3955 - mean_absolute_error: 0.7578 - val_loss: 0.3707 - val_mean_absolute_error: 0.7267\n",
      "Epoch 59/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3872 - mean_absolute_error: 0.7456WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3900 - mean_absolute_error: 0.7493 - val_loss: 0.3695 - val_mean_absolute_error: 0.7264\n",
      "Epoch 60/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4024 - mean_absolute_error: 0.7663WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4009 - mean_absolute_error: 0.7647 - val_loss: 0.3787 - val_mean_absolute_error: 0.7422\n",
      "Epoch 61/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.4071 - mean_absolute_error: 0.7752WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4016 - mean_absolute_error: 0.7666 - val_loss: 0.4126 - val_mean_absolute_error: 0.7680\n",
      "Epoch 62/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3967 - mean_absolute_error: 0.7622WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3931 - mean_absolute_error: 0.7583 - val_loss: 0.3679 - val_mean_absolute_error: 0.7248\n",
      "Epoch 63/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3972 - mean_absolute_error: 0.7593WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3956 - mean_absolute_error: 0.7579 - val_loss: 0.4105 - val_mean_absolute_error: 0.7633\n",
      "Epoch 64/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3974 - mean_absolute_error: 0.7574WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3937 - mean_absolute_error: 0.7534 - val_loss: 0.3717 - val_mean_absolute_error: 0.7259\n",
      "Epoch 65/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3908 - mean_absolute_error: 0.7519WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3847 - mean_absolute_error: 0.7446 - val_loss: 0.3899 - val_mean_absolute_error: 0.7420\n",
      "Epoch 66/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.3926 - mean_absolute_error: 0.7563WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3938 - mean_absolute_error: 0.7578 - val_loss: 0.3692 - val_mean_absolute_error: 0.7222\n",
      "Epoch 67/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3899 - mean_absolute_error: 0.7497WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3864 - mean_absolute_error: 0.7446 - val_loss: 0.3979 - val_mean_absolute_error: 0.7499\n",
      "Epoch 68/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3906 - mean_absolute_error: 0.7526WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3905 - mean_absolute_error: 0.7527 - val_loss: 0.4285 - val_mean_absolute_error: 0.7846\n",
      "Epoch 69/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.3817 - mean_absolute_error: 0.7418WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3850 - mean_absolute_error: 0.7437 - val_loss: 0.3915 - val_mean_absolute_error: 0.7460\n",
      "Epoch 70/200\n",
      "46/61 [=====================>........] - ETA: 0s - loss: 0.3733 - mean_absolute_error: 0.7309WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3780 - mean_absolute_error: 0.7365 - val_loss: 0.4077 - val_mean_absolute_error: 0.7598\n",
      "Epoch 71/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3730 - mean_absolute_error: 0.7298WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3709 - mean_absolute_error: 0.7266 - val_loss: 0.3782 - val_mean_absolute_error: 0.7418\n",
      "Epoch 72/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.3723 - mean_absolute_error: 0.7321WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3749 - mean_absolute_error: 0.7355 - val_loss: 0.3889 - val_mean_absolute_error: 0.7561\n",
      "Epoch 73/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3525 - mean_absolute_error: 0.7053WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3509 - mean_absolute_error: 0.7030 - val_loss: 0.4297 - val_mean_absolute_error: 0.8017\n",
      "Epoch 74/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3479 - mean_absolute_error: 0.6981WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3473 - mean_absolute_error: 0.6987 - val_loss: 0.3278 - val_mean_absolute_error: 0.6745\n",
      "Epoch 75/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3419 - mean_absolute_error: 0.6943WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3446 - mean_absolute_error: 0.6973 - val_loss: 0.4120 - val_mean_absolute_error: 0.7788\n",
      "Epoch 76/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3283 - mean_absolute_error: 0.6748WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3344 - mean_absolute_error: 0.6816 - val_loss: 0.3380 - val_mean_absolute_error: 0.6894\n",
      "Epoch 77/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3196 - mean_absolute_error: 0.6608WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3229 - mean_absolute_error: 0.6631 - val_loss: 0.4035 - val_mean_absolute_error: 0.7762\n",
      "Epoch 78/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3335 - mean_absolute_error: 0.6788WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3280 - mean_absolute_error: 0.6718 - val_loss: 0.3246 - val_mean_absolute_error: 0.6673\n",
      "Epoch 79/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3182 - mean_absolute_error: 0.6629WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3203 - mean_absolute_error: 0.6643 - val_loss: 0.3155 - val_mean_absolute_error: 0.6556\n",
      "Epoch 80/200\n",
      "36/61 [================>.............] - ETA: 0s - loss: 0.3325 - mean_absolute_error: 0.6783WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.3236 - mean_absolute_error: 0.6697 - val_loss: 0.3166 - val_mean_absolute_error: 0.6600\n",
      "Epoch 81/200\n",
      "45/61 [=====================>........] - ETA: 0s - loss: 0.3136 - mean_absolute_error: 0.6540WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3149 - mean_absolute_error: 0.6563 - val_loss: 0.3575 - val_mean_absolute_error: 0.7012\n",
      "Epoch 82/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3263 - mean_absolute_error: 0.6669WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3245 - mean_absolute_error: 0.6648 - val_loss: 0.3011 - val_mean_absolute_error: 0.6361\n",
      "Epoch 83/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3281 - mean_absolute_error: 0.6742WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3314 - mean_absolute_error: 0.6776 - val_loss: 0.3088 - val_mean_absolute_error: 0.6480\n",
      "Epoch 84/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3134 - mean_absolute_error: 0.6519WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3116 - mean_absolute_error: 0.6501 - val_loss: 0.3309 - val_mean_absolute_error: 0.6756\n",
      "Epoch 85/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3131 - mean_absolute_error: 0.6557WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3180 - mean_absolute_error: 0.6608 - val_loss: 0.3540 - val_mean_absolute_error: 0.7039\n",
      "Epoch 86/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3193 - mean_absolute_error: 0.6612WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3197 - mean_absolute_error: 0.6622 - val_loss: 0.2985 - val_mean_absolute_error: 0.6358\n",
      "Epoch 87/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3184 - mean_absolute_error: 0.6599WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3169 - mean_absolute_error: 0.6589 - val_loss: 0.3264 - val_mean_absolute_error: 0.6707\n",
      "Epoch 88/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3181 - mean_absolute_error: 0.6574WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3157 - mean_absolute_error: 0.6550 - val_loss: 0.3074 - val_mean_absolute_error: 0.6457\n",
      "Epoch 89/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3132 - mean_absolute_error: 0.6570WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3133 - mean_absolute_error: 0.6569 - val_loss: 0.3277 - val_mean_absolute_error: 0.6705\n",
      "Epoch 90/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3168 - mean_absolute_error: 0.6580WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3172 - mean_absolute_error: 0.6597 - val_loss: 0.3043 - val_mean_absolute_error: 0.6407\n",
      "Epoch 91/200\n",
      "46/61 [=====================>........] - ETA: 0s - loss: 0.3245 - mean_absolute_error: 0.6675WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3121 - mean_absolute_error: 0.6529 - val_loss: 0.3006 - val_mean_absolute_error: 0.6429\n",
      "Epoch 92/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3085 - mean_absolute_error: 0.6467WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3134 - mean_absolute_error: 0.6527 - val_loss: 0.2900 - val_mean_absolute_error: 0.6174\n",
      "Epoch 93/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3111 - mean_absolute_error: 0.6488WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3093 - mean_absolute_error: 0.6478 - val_loss: 0.3144 - val_mean_absolute_error: 0.6576\n",
      "Epoch 94/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3134 - mean_absolute_error: 0.6506WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3155 - mean_absolute_error: 0.6528 - val_loss: 0.2914 - val_mean_absolute_error: 0.6241\n",
      "Epoch 95/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3063 - mean_absolute_error: 0.6405WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3075 - mean_absolute_error: 0.6416 - val_loss: 0.3082 - val_mean_absolute_error: 0.6464\n",
      "Epoch 96/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3116 - mean_absolute_error: 0.6512WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3088 - mean_absolute_error: 0.6480 - val_loss: 0.3084 - val_mean_absolute_error: 0.6478\n",
      "Epoch 97/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3083 - mean_absolute_error: 0.6460WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3080 - mean_absolute_error: 0.6454 - val_loss: 0.3225 - val_mean_absolute_error: 0.6697\n",
      "Epoch 98/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3001 - mean_absolute_error: 0.6363WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3036 - mean_absolute_error: 0.6405 - val_loss: 0.3095 - val_mean_absolute_error: 0.6508\n",
      "Epoch 99/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3086 - mean_absolute_error: 0.6486WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3099 - mean_absolute_error: 0.6499 - val_loss: 0.2940 - val_mean_absolute_error: 0.6284\n",
      "Epoch 100/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2986 - mean_absolute_error: 0.6334WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.3041 - mean_absolute_error: 0.6414 - val_loss: 0.2942 - val_mean_absolute_error: 0.6258\n",
      "Epoch 101/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3058 - mean_absolute_error: 0.6424WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3086 - mean_absolute_error: 0.6462 - val_loss: 0.3426 - val_mean_absolute_error: 0.6805\n",
      "Epoch 102/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2959 - mean_absolute_error: 0.6291WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2962 - mean_absolute_error: 0.6302 - val_loss: 0.2982 - val_mean_absolute_error: 0.6377\n",
      "Epoch 103/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3097 - mean_absolute_error: 0.6495WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3055 - mean_absolute_error: 0.6419 - val_loss: 0.3038 - val_mean_absolute_error: 0.6367\n",
      "Epoch 104/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3104 - mean_absolute_error: 0.6481WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3116 - mean_absolute_error: 0.6510 - val_loss: 0.2969 - val_mean_absolute_error: 0.6348\n",
      "Epoch 105/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3032 - mean_absolute_error: 0.6405WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3065 - mean_absolute_error: 0.6453 - val_loss: 0.3284 - val_mean_absolute_error: 0.6799\n",
      "Epoch 106/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3038 - mean_absolute_error: 0.6417WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3099 - mean_absolute_error: 0.6494 - val_loss: 0.3100 - val_mean_absolute_error: 0.6558\n",
      "Epoch 107/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3018 - mean_absolute_error: 0.6369WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3007 - mean_absolute_error: 0.6362 - val_loss: 0.2999 - val_mean_absolute_error: 0.6396\n",
      "Epoch 108/200\n",
      "44/61 [====================>.........] - ETA: 0s - loss: 0.2965 - mean_absolute_error: 0.6327WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3016 - mean_absolute_error: 0.6396 - val_loss: 0.3049 - val_mean_absolute_error: 0.6469\n",
      "Epoch 109/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3126 - mean_absolute_error: 0.6500WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3114 - mean_absolute_error: 0.6476 - val_loss: 0.3475 - val_mean_absolute_error: 0.7085\n",
      "Epoch 110/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3022 - mean_absolute_error: 0.6360WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3026 - mean_absolute_error: 0.6368 - val_loss: 0.2852 - val_mean_absolute_error: 0.6144\n",
      "Epoch 111/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2986 - mean_absolute_error: 0.6323WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3045 - mean_absolute_error: 0.6399 - val_loss: 0.2884 - val_mean_absolute_error: 0.6190\n",
      "Epoch 112/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3008 - mean_absolute_error: 0.6362WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3004 - mean_absolute_error: 0.6336 - val_loss: 0.3226 - val_mean_absolute_error: 0.6699\n",
      "Epoch 113/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3021 - mean_absolute_error: 0.6390WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3031 - mean_absolute_error: 0.6400 - val_loss: 0.3074 - val_mean_absolute_error: 0.6505\n",
      "Epoch 114/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3066 - mean_absolute_error: 0.6468WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3053 - mean_absolute_error: 0.6461 - val_loss: 0.3776 - val_mean_absolute_error: 0.7316\n",
      "Epoch 115/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.3003 - mean_absolute_error: 0.6344WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3025 - mean_absolute_error: 0.6366 - val_loss: 0.3034 - val_mean_absolute_error: 0.6439\n",
      "Epoch 116/200\n",
      "44/61 [====================>.........] - ETA: 0s - loss: 0.3063 - mean_absolute_error: 0.6430WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3025 - mean_absolute_error: 0.6392 - val_loss: 0.3124 - val_mean_absolute_error: 0.6591\n",
      "Epoch 117/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3053 - mean_absolute_error: 0.6415WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3052 - mean_absolute_error: 0.6415 - val_loss: 0.3088 - val_mean_absolute_error: 0.6558\n",
      "Epoch 118/200\n",
      "41/61 [===================>..........] - ETA: 0s - loss: 0.3121 - mean_absolute_error: 0.6505WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.3071 - mean_absolute_error: 0.6444 - val_loss: 0.2967 - val_mean_absolute_error: 0.6363\n",
      "Epoch 119/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3025 - mean_absolute_error: 0.6409WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3050 - mean_absolute_error: 0.6433 - val_loss: 0.2944 - val_mean_absolute_error: 0.6309\n",
      "Epoch 120/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2967 - mean_absolute_error: 0.6332WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2971 - mean_absolute_error: 0.6334 - val_loss: 0.2907 - val_mean_absolute_error: 0.6230\n",
      "Epoch 121/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.3120 - mean_absolute_error: 0.6491WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3062 - mean_absolute_error: 0.6439 - val_loss: 0.2980 - val_mean_absolute_error: 0.6295\n",
      "Epoch 122/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.3051 - mean_absolute_error: 0.6412WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3056 - mean_absolute_error: 0.6422 - val_loss: 0.3321 - val_mean_absolute_error: 0.6843\n",
      "Epoch 123/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3004 - mean_absolute_error: 0.6375WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3004 - mean_absolute_error: 0.6365 - val_loss: 0.3079 - val_mean_absolute_error: 0.6504\n",
      "Epoch 124/200\n",
      "49/61 [=======================>......] - ETA: 0s - loss: 0.3046 - mean_absolute_error: 0.6428WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3023 - mean_absolute_error: 0.6386 - val_loss: 0.3011 - val_mean_absolute_error: 0.6372\n",
      "Epoch 125/200\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 0.2955 - mean_absolute_error: 0.6334WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3022 - mean_absolute_error: 0.6403 - val_loss: 0.3021 - val_mean_absolute_error: 0.6423\n",
      "Epoch 126/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3003 - mean_absolute_error: 0.6374WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2981 - mean_absolute_error: 0.6345 - val_loss: 0.2891 - val_mean_absolute_error: 0.6260\n",
      "Epoch 127/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3089 - mean_absolute_error: 0.6452WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3034 - mean_absolute_error: 0.6379 - val_loss: 0.3224 - val_mean_absolute_error: 0.6715\n",
      "Epoch 128/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2998 - mean_absolute_error: 0.6345WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3005 - mean_absolute_error: 0.6342 - val_loss: 0.2971 - val_mean_absolute_error: 0.6341\n",
      "Epoch 129/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3009 - mean_absolute_error: 0.6386WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3008 - mean_absolute_error: 0.6384 - val_loss: 0.3024 - val_mean_absolute_error: 0.6429\n",
      "Epoch 130/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.3016 - mean_absolute_error: 0.6374WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2996 - mean_absolute_error: 0.6336 - val_loss: 0.3049 - val_mean_absolute_error: 0.6442\n",
      "Epoch 131/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2970 - mean_absolute_error: 0.6308WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2953 - mean_absolute_error: 0.6290 - val_loss: 0.3203 - val_mean_absolute_error: 0.6673\n",
      "Epoch 132/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.3009 - mean_absolute_error: 0.6358WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2985 - mean_absolute_error: 0.6339 - val_loss: 0.2966 - val_mean_absolute_error: 0.6345\n",
      "Epoch 133/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2946 - mean_absolute_error: 0.6301WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2972 - mean_absolute_error: 0.6330 - val_loss: 0.2852 - val_mean_absolute_error: 0.6197\n",
      "Epoch 134/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2962 - mean_absolute_error: 0.6313WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2994 - mean_absolute_error: 0.6365 - val_loss: 0.2920 - val_mean_absolute_error: 0.6315\n",
      "Epoch 135/200\n",
      "43/61 [====================>.........] - ETA: 0s - loss: 0.2936 - mean_absolute_error: 0.6279WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2948 - mean_absolute_error: 0.6295 - val_loss: 0.3095 - val_mean_absolute_error: 0.6561\n",
      "Epoch 136/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.3091 - mean_absolute_error: 0.6496WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3020 - mean_absolute_error: 0.6420 - val_loss: 0.3042 - val_mean_absolute_error: 0.6314\n",
      "Epoch 137/200\n",
      "43/61 [====================>.........] - ETA: 0s - loss: 0.3033 - mean_absolute_error: 0.6388WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3074 - mean_absolute_error: 0.6446 - val_loss: 0.2913 - val_mean_absolute_error: 0.6271\n",
      "Epoch 138/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2996 - mean_absolute_error: 0.6360WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2987 - mean_absolute_error: 0.6346 - val_loss: 0.2937 - val_mean_absolute_error: 0.6259\n",
      "Epoch 139/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.3007 - mean_absolute_error: 0.6361WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3029 - mean_absolute_error: 0.6388 - val_loss: 0.3452 - val_mean_absolute_error: 0.7032\n",
      "Epoch 140/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.3041 - mean_absolute_error: 0.6395WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3030 - mean_absolute_error: 0.6384 - val_loss: 0.3149 - val_mean_absolute_error: 0.6598\n",
      "Epoch 141/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2936 - mean_absolute_error: 0.6239WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2917 - mean_absolute_error: 0.6221 - val_loss: 0.3195 - val_mean_absolute_error: 0.6639\n",
      "Epoch 142/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2877 - mean_absolute_error: 0.6197WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2908 - mean_absolute_error: 0.6235 - val_loss: 0.2968 - val_mean_absolute_error: 0.6384\n",
      "Epoch 143/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2889 - mean_absolute_error: 0.6177WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2937 - mean_absolute_error: 0.6246 - val_loss: 0.3153 - val_mean_absolute_error: 0.6625\n",
      "Epoch 144/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.3016 - mean_absolute_error: 0.6408WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2976 - mean_absolute_error: 0.6352 - val_loss: 0.3426 - val_mean_absolute_error: 0.6991\n",
      "Epoch 145/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2997 - mean_absolute_error: 0.6359WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2973 - mean_absolute_error: 0.6333 - val_loss: 0.3034 - val_mean_absolute_error: 0.6427\n",
      "Epoch 146/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2930 - mean_absolute_error: 0.6243WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2933 - mean_absolute_error: 0.6239 - val_loss: 0.2892 - val_mean_absolute_error: 0.6249\n",
      "Epoch 147/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2883 - mean_absolute_error: 0.6219WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2899 - mean_absolute_error: 0.6238 - val_loss: 0.3054 - val_mean_absolute_error: 0.6485\n",
      "Epoch 148/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2904 - mean_absolute_error: 0.6271WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2960 - mean_absolute_error: 0.6328 - val_loss: 0.3114 - val_mean_absolute_error: 0.6533\n",
      "Epoch 149/200\n",
      "30/61 [=============>................] - ETA: 0s - loss: 0.3004 - mean_absolute_error: 0.6381WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2944 - mean_absolute_error: 0.6298 - val_loss: 0.3034 - val_mean_absolute_error: 0.6423\n",
      "Epoch 150/200\n",
      "45/61 [=====================>........] - ETA: 0s - loss: 0.3105 - mean_absolute_error: 0.6498WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2995 - mean_absolute_error: 0.6360 - val_loss: 0.2817 - val_mean_absolute_error: 0.6142\n",
      "Epoch 151/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.2899 - mean_absolute_error: 0.6211WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2910 - mean_absolute_error: 0.6228 - val_loss: 0.2979 - val_mean_absolute_error: 0.6379\n",
      "Epoch 152/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2955 - mean_absolute_error: 0.6330WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2932 - mean_absolute_error: 0.6297 - val_loss: 0.2852 - val_mean_absolute_error: 0.6158\n",
      "Epoch 153/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.2967 - mean_absolute_error: 0.6295WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2986 - mean_absolute_error: 0.6327 - val_loss: 0.2905 - val_mean_absolute_error: 0.6292\n",
      "Epoch 154/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.2936 - mean_absolute_error: 0.6258WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2963 - mean_absolute_error: 0.6299 - val_loss: 0.3032 - val_mean_absolute_error: 0.6452\n",
      "Epoch 155/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2809 - mean_absolute_error: 0.6095WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2869 - mean_absolute_error: 0.6190 - val_loss: 0.3281 - val_mean_absolute_error: 0.6830\n",
      "Epoch 156/200\n",
      "45/61 [=====================>........] - ETA: 0s - loss: 0.2953 - mean_absolute_error: 0.6305WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3035 - mean_absolute_error: 0.6411 - val_loss: 0.2983 - val_mean_absolute_error: 0.6382\n",
      "Epoch 157/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2895 - mean_absolute_error: 0.6207WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2918 - mean_absolute_error: 0.6242 - val_loss: 0.2858 - val_mean_absolute_error: 0.6191\n",
      "Epoch 158/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2943 - mean_absolute_error: 0.6269WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2950 - mean_absolute_error: 0.6286 - val_loss: 0.2862 - val_mean_absolute_error: 0.6190\n",
      "Epoch 159/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2852 - mean_absolute_error: 0.6146WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2900 - mean_absolute_error: 0.6214 - val_loss: 0.3143 - val_mean_absolute_error: 0.6643\n",
      "Epoch 160/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.3066 - mean_absolute_error: 0.6469WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3020 - mean_absolute_error: 0.6422 - val_loss: 0.2928 - val_mean_absolute_error: 0.6324\n",
      "Epoch 161/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2866 - mean_absolute_error: 0.6145WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2899 - mean_absolute_error: 0.6185 - val_loss: 0.3080 - val_mean_absolute_error: 0.6501\n",
      "Epoch 162/200\n",
      "44/61 [====================>.........] - ETA: 0s - loss: 0.2877 - mean_absolute_error: 0.6157WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2916 - mean_absolute_error: 0.6228 - val_loss: 0.3383 - val_mean_absolute_error: 0.6978\n",
      "Epoch 163/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2932 - mean_absolute_error: 0.6259WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2922 - mean_absolute_error: 0.6242 - val_loss: 0.2922 - val_mean_absolute_error: 0.6220\n",
      "Epoch 164/200\n",
      "38/61 [=================>............] - ETA: 0s - loss: 0.2995 - mean_absolute_error: 0.6376WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2955 - mean_absolute_error: 0.6325 - val_loss: 0.2855 - val_mean_absolute_error: 0.6208\n",
      "Epoch 165/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2929 - mean_absolute_error: 0.6247WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2926 - mean_absolute_error: 0.6246 - val_loss: 0.2947 - val_mean_absolute_error: 0.6313\n",
      "Epoch 166/200\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 0.2848 - mean_absolute_error: 0.6159WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2937 - mean_absolute_error: 0.6263 - val_loss: 0.2889 - val_mean_absolute_error: 0.6279\n",
      "Epoch 167/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2862 - mean_absolute_error: 0.6204WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2909 - mean_absolute_error: 0.6250 - val_loss: 0.2895 - val_mean_absolute_error: 0.6281\n",
      "Epoch 168/200\n",
      "50/61 [=======================>......] - ETA: 0s - loss: 0.2894 - mean_absolute_error: 0.6212WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2879 - mean_absolute_error: 0.6192 - val_loss: 0.3172 - val_mean_absolute_error: 0.6685\n",
      "Epoch 169/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2893 - mean_absolute_error: 0.6227WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2982 - mean_absolute_error: 0.6350 - val_loss: 0.3022 - val_mean_absolute_error: 0.6440\n",
      "Epoch 170/200\n",
      "49/61 [=======================>......] - ETA: 0s - loss: 0.2932 - mean_absolute_error: 0.6250WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2877 - mean_absolute_error: 0.6183 - val_loss: 0.2879 - val_mean_absolute_error: 0.6189\n",
      "Epoch 171/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2885 - mean_absolute_error: 0.6212WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2893 - mean_absolute_error: 0.6219 - val_loss: 0.3079 - val_mean_absolute_error: 0.6485\n",
      "Epoch 172/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2861 - mean_absolute_error: 0.6202WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2899 - mean_absolute_error: 0.6243 - val_loss: 0.2878 - val_mean_absolute_error: 0.6266\n",
      "Epoch 173/200\n",
      "46/61 [=====================>........] - ETA: 0s - loss: 0.2954 - mean_absolute_error: 0.6281WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2901 - mean_absolute_error: 0.6224 - val_loss: 0.2856 - val_mean_absolute_error: 0.6187\n",
      "Epoch 174/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2928 - mean_absolute_error: 0.6238WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2897 - mean_absolute_error: 0.6206 - val_loss: 0.2807 - val_mean_absolute_error: 0.6101\n",
      "Epoch 175/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2952 - mean_absolute_error: 0.6288WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2965 - mean_absolute_error: 0.6301 - val_loss: 0.2988 - val_mean_absolute_error: 0.6386\n",
      "Epoch 176/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.2786 - mean_absolute_error: 0.6098WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2840 - mean_absolute_error: 0.6160 - val_loss: 0.2960 - val_mean_absolute_error: 0.6367\n",
      "Epoch 177/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2847 - mean_absolute_error: 0.6143WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2912 - mean_absolute_error: 0.6225 - val_loss: 0.3053 - val_mean_absolute_error: 0.6431\n",
      "Epoch 178/200\n",
      "30/61 [=============>................] - ETA: 0s - loss: 0.2848 - mean_absolute_error: 0.6102WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_absolute_error: 0.6241 - val_loss: 0.2895 - val_mean_absolute_error: 0.6194\n",
      "Epoch 179/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.2960 - mean_absolute_error: 0.6300WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2939 - mean_absolute_error: 0.6269 - val_loss: 0.2852 - val_mean_absolute_error: 0.6208\n",
      "Epoch 180/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.2863 - mean_absolute_error: 0.6158WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2853 - mean_absolute_error: 0.6131 - val_loss: 0.3135 - val_mean_absolute_error: 0.6589\n",
      "Epoch 181/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2924 - mean_absolute_error: 0.6249WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2929 - mean_absolute_error: 0.6250 - val_loss: 0.2961 - val_mean_absolute_error: 0.6367\n",
      "Epoch 182/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.2955 - mean_absolute_error: 0.6303WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2878 - mean_absolute_error: 0.6238 - val_loss: 0.2977 - val_mean_absolute_error: 0.6295\n",
      "Epoch 183/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2962 - mean_absolute_error: 0.6285WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2946 - mean_absolute_error: 0.6276 - val_loss: 0.2912 - val_mean_absolute_error: 0.6224\n",
      "Epoch 184/200\n",
      "41/61 [===================>..........] - ETA: 0s - loss: 0.2873 - mean_absolute_error: 0.6175WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2871 - mean_absolute_error: 0.6204 - val_loss: 0.2990 - val_mean_absolute_error: 0.6382\n",
      "Epoch 185/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2917 - mean_absolute_error: 0.6267WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2906 - mean_absolute_error: 0.6263 - val_loss: 0.3440 - val_mean_absolute_error: 0.7036\n",
      "Epoch 186/200\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.2886 - mean_absolute_error: 0.6214WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2913 - mean_absolute_error: 0.6249 - val_loss: 0.2849 - val_mean_absolute_error: 0.6162\n",
      "Epoch 187/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2893 - mean_absolute_error: 0.6201WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2910 - mean_absolute_error: 0.6210 - val_loss: 0.3021 - val_mean_absolute_error: 0.6460\n",
      "Epoch 188/200\n",
      "58/61 [===========================>..] - ETA: 0s - loss: 0.2920 - mean_absolute_error: 0.6257WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2911 - mean_absolute_error: 0.6238 - val_loss: 0.2948 - val_mean_absolute_error: 0.6325\n",
      "Epoch 189/200\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.2781 - mean_absolute_error: 0.6079WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2819 - mean_absolute_error: 0.6109 - val_loss: 0.3005 - val_mean_absolute_error: 0.6429\n",
      "Epoch 190/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2893 - mean_absolute_error: 0.6185WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2929 - mean_absolute_error: 0.6222 - val_loss: 0.2847 - val_mean_absolute_error: 0.6157\n",
      "Epoch 191/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2889 - mean_absolute_error: 0.6201WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2886 - mean_absolute_error: 0.6195 - val_loss: 0.3171 - val_mean_absolute_error: 0.6505\n",
      "Epoch 192/200\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.2875 - mean_absolute_error: 0.6218WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2854 - mean_absolute_error: 0.6195 - val_loss: 0.3191 - val_mean_absolute_error: 0.6717\n",
      "Epoch 193/200\n",
      "34/61 [===============>..............] - ETA: 0s - loss: 0.3039 - mean_absolute_error: 0.6348WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2934 - mean_absolute_error: 0.6257 - val_loss: 0.2948 - val_mean_absolute_error: 0.6247\n",
      "Epoch 194/200\n",
      "39/61 [==================>...........] - ETA: 0s - loss: 0.2922 - mean_absolute_error: 0.6285WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2926 - mean_absolute_error: 0.6263 - val_loss: 0.2925 - val_mean_absolute_error: 0.6311\n",
      "Epoch 195/200\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.2933 - mean_absolute_error: 0.6276WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2901 - mean_absolute_error: 0.6221 - val_loss: 0.2908 - val_mean_absolute_error: 0.6297\n",
      "Epoch 196/200\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.2873 - mean_absolute_error: 0.6208WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2884 - mean_absolute_error: 0.6201 - val_loss: 0.2982 - val_mean_absolute_error: 0.6340\n",
      "Epoch 197/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.2828 - mean_absolute_error: 0.6130WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2849 - mean_absolute_error: 0.6161 - val_loss: 0.2945 - val_mean_absolute_error: 0.6325\n",
      "Epoch 198/200\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.2963 - mean_absolute_error: 0.6294WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2937 - mean_absolute_error: 0.6256 - val_loss: 0.2794 - val_mean_absolute_error: 0.6088\n",
      "Epoch 199/200\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.2889 - mean_absolute_error: 0.6220WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2884 - mean_absolute_error: 0.6218 - val_loss: 0.2790 - val_mean_absolute_error: 0.6095\n",
      "Epoch 200/200\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.2854 - mean_absolute_error: 0.6147WARNING:tensorflow:Early stopping conditioned on metric `huber_loss` which is not available. Available metrics are: loss,mean_absolute_error,val_loss,val_mean_absolute_error\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_absolute_error: 0.6162 - val_loss: 0.2896 - val_mean_absolute_error: 0.6265\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(7,), activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "\t\tDropout(0.2),\n",
    "\t\tBatchNormalization(),\n",
    "    Dense(128, activation='tanh'),\n",
    "\n",
    "    Dense(1) \n",
    "])\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='huber_loss', \n",
    "    patience=20, \n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam', loss='huber_loss', \n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "batched_history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    validation_split=0.25, \n",
    "    batch_size=40, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"605.803125pt\" height=\"392.514375pt\" viewBox=\"0 0 605.803125 392.514375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-08-06T14:48:06.304011</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 392.514375 \n",
       "L 605.803125 392.514375 \n",
       "L 605.803125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 354.958125 \n",
       "L 598.603125 354.958125 \n",
       "L 598.603125 22.318125 \n",
       "L 40.603125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m501863096c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"63.417652\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(60.236402 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"127.145382\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(120.782882 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"190.873111\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(184.510611 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"254.600841\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 75 -->\n",
       "      <g transform=\"translate(248.238341 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"318.32857\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(308.78482 369.556562) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"382.0563\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 125 -->\n",
       "      <g transform=\"translate(372.51255 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"445.78403\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(436.24028 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"509.511759\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 175 -->\n",
       "      <g transform=\"translate(499.968009 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m501863096c\" x=\"573.239489\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(563.695739 369.556562) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Epochs -->\n",
       "     <g transform=\"translate(301.6875 383.234687) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"306.201172\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"meb252fba86\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"354.958125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 358.757344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(27.240625 292.229344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"221.902125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(27.240625 225.701344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"155.374125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(27.240625 159.173344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"88.846125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(27.240625 92.645344) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#meb252fba86\" x=\"40.603125\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 26.117344) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 198.295937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 65.966761 224.523264 \n",
       "L 68.515871 312.434666 \n",
       "L 71.06498 335.47944 \n",
       "L 73.614089 336.816379 \n",
       "L 76.163198 339.39919 \n",
       "L 78.712307 339.067061 \n",
       "L 83.810526 340.144245 \n",
       "L 86.359635 339.9799 \n",
       "L 88.908744 340.824962 \n",
       "L 91.457853 340.899558 \n",
       "L 94.006962 340.187702 \n",
       "L 96.556072 340.328804 \n",
       "L 99.105181 340.238084 \n",
       "L 101.65429 340.813247 \n",
       "L 104.203399 340.893186 \n",
       "L 106.752508 340.71162 \n",
       "L 109.301617 341.195086 \n",
       "L 111.850727 340.546468 \n",
       "L 114.399836 340.710827 \n",
       "L 116.948945 341.219394 \n",
       "L 119.498054 340.804274 \n",
       "L 122.047163 340.887455 \n",
       "L 124.596273 340.581144 \n",
       "L 127.145382 341.212292 \n",
       "L 129.694491 340.954039 \n",
       "L 132.2436 340.931108 \n",
       "L 134.792709 341.079826 \n",
       "L 139.890928 341.006822 \n",
       "L 142.440037 341.210618 \n",
       "L 144.989146 341.191608 \n",
       "L 147.538255 341.372534 \n",
       "L 150.087364 341.190203 \n",
       "L 152.636474 341.563062 \n",
       "L 155.185583 341.411014 \n",
       "L 160.283801 341.314902 \n",
       "L 162.83291 341.646296 \n",
       "L 165.382019 341.545897 \n",
       "L 167.931129 341.563932 \n",
       "L 170.480238 341.746839 \n",
       "L 175.578456 341.456185 \n",
       "L 178.127565 341.531377 \n",
       "L 180.676675 341.741487 \n",
       "L 183.225784 341.297613 \n",
       "L 185.774893 341.268168 \n",
       "L 188.324002 341.775737 \n",
       "L 193.42222 341.640239 \n",
       "L 195.97133 341.970777 \n",
       "L 198.520439 341.477497 \n",
       "L 201.069548 341.856266 \n",
       "L 203.618657 341.547915 \n",
       "L 206.167766 341.703456 \n",
       "L 208.716876 341.510337 \n",
       "L 213.815094 341.98526 \n",
       "L 216.364203 341.62188 \n",
       "L 218.913312 341.600298 \n",
       "L 221.462421 341.883114 \n",
       "L 224.011531 341.799133 \n",
       "L 226.56064 341.861885 \n",
       "L 229.109749 342.162476 \n",
       "L 231.658858 341.858386 \n",
       "L 234.207967 342.10585 \n",
       "L 236.757077 341.968805 \n",
       "L 244.404404 342.621756 \n",
       "L 246.953513 342.487104 \n",
       "L 249.502622 343.286826 \n",
       "L 254.600841 343.496503 \n",
       "L 259.699059 344.216709 \n",
       "L 262.248168 344.047302 \n",
       "L 264.797278 344.303412 \n",
       "L 267.346387 344.193914 \n",
       "L 269.895496 344.483291 \n",
       "L 274.993714 343.933007 \n",
       "L 277.542823 344.592012 \n",
       "L 280.091933 344.3799 \n",
       "L 282.641042 344.324022 \n",
       "L 290.288369 344.535057 \n",
       "L 292.837479 344.406171 \n",
       "L 295.386588 344.576346 \n",
       "L 297.935697 344.532932 \n",
       "L 300.484806 344.669955 \n",
       "L 303.033915 344.462148 \n",
       "L 305.583024 344.727957 \n",
       "L 310.681243 344.712837 \n",
       "L 313.230352 344.860365 \n",
       "L 315.779461 344.650382 \n",
       "L 318.32857 344.841312 \n",
       "L 320.87768 344.692948 \n",
       "L 323.426789 345.10409 \n",
       "L 328.525007 344.594405 \n",
       "L 331.074116 344.763468 \n",
       "L 333.623226 344.650939 \n",
       "L 336.172335 344.957249 \n",
       "L 338.721444 344.92419 \n",
       "L 341.270553 344.598742 \n",
       "L 343.819662 344.892547 \n",
       "L 346.368771 344.830349 \n",
       "L 348.917881 344.966174 \n",
       "L 354.016099 344.803198 \n",
       "L 359.114317 344.895321 \n",
       "L 364.212536 344.741871 \n",
       "L 366.761645 344.812704 \n",
       "L 369.310754 345.075922 \n",
       "L 371.859863 344.773594 \n",
       "L 374.408972 344.793987 \n",
       "L 376.958082 344.964907 \n",
       "L 382.0563 344.907014 \n",
       "L 384.605409 345.043601 \n",
       "L 387.154518 344.866538 \n",
       "L 392.252737 344.952801 \n",
       "L 402.449173 345.071463 \n",
       "L 404.998283 344.999729 \n",
       "L 407.547392 345.152864 \n",
       "L 412.64561 344.732842 \n",
       "L 415.194719 345.02342 \n",
       "L 417.743829 344.881679 \n",
       "L 420.292938 344.880205 \n",
       "L 422.842047 345.25621 \n",
       "L 425.391156 345.28441 \n",
       "L 433.038484 345.067841 \n",
       "L 438.136702 345.316175 \n",
       "L 440.685811 345.113143 \n",
       "L 443.23492 345.164422 \n",
       "L 445.78403 344.995383 \n",
       "L 448.333139 345.276944 \n",
       "L 455.980466 345.100548 \n",
       "L 458.529575 345.413964 \n",
       "L 461.078685 344.862146 \n",
       "L 463.627794 345.250535 \n",
       "L 466.176903 345.145011 \n",
       "L 468.726012 345.312702 \n",
       "L 471.275121 344.9124 \n",
       "L 473.824231 345.315 \n",
       "L 481.471558 345.128126 \n",
       "L 484.020667 345.224014 \n",
       "L 486.569776 345.1873 \n",
       "L 491.667995 345.38128 \n",
       "L 494.217104 345.039454 \n",
       "L 496.766213 345.387228 \n",
       "L 504.413541 345.307534 \n",
       "L 506.96265 345.323115 \n",
       "L 509.511759 345.096518 \n",
       "L 512.060868 345.510572 \n",
       "L 514.609977 345.273002 \n",
       "L 519.708196 345.181002 \n",
       "L 522.257305 345.466695 \n",
       "L 524.806414 345.21555 \n",
       "L 527.355523 345.383671 \n",
       "L 529.904633 345.159241 \n",
       "L 532.453742 345.407961 \n",
       "L 537.55196 345.269129 \n",
       "L 542.650178 345.27446 \n",
       "L 545.199288 345.580343 \n",
       "L 547.748397 345.213483 \n",
       "L 552.846615 345.464663 \n",
       "L 555.395724 345.199379 \n",
       "L 563.043052 345.365931 \n",
       "L 565.592161 345.481238 \n",
       "L 568.14127 345.188716 \n",
       "L 570.690379 345.363904 \n",
       "L 573.239489 345.427636 \n",
       "L 573.239489 345.427636 \n",
       "\" clip-path=\"url(#pe9dde70291)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 65.966761 213.684546 \n",
       "L 68.515871 337.273765 \n",
       "L 71.06498 340.56608 \n",
       "L 73.614089 341.167572 \n",
       "L 76.163198 340.889279 \n",
       "L 78.712307 341.576654 \n",
       "L 81.261416 341.947171 \n",
       "L 83.810526 341.232629 \n",
       "L 86.359635 341.55863 \n",
       "L 88.908744 340.996307 \n",
       "L 91.457853 342.067551 \n",
       "L 96.556072 342.174561 \n",
       "L 99.105181 341.827397 \n",
       "L 101.65429 342.013541 \n",
       "L 104.203399 342.335411 \n",
       "L 106.752508 342.309716 \n",
       "L 109.301617 342.010896 \n",
       "L 111.850727 342.084508 \n",
       "L 116.948945 341.764427 \n",
       "L 119.498054 341.799266 \n",
       "L 122.047163 342.165998 \n",
       "L 124.596273 342.082937 \n",
       "L 129.694491 342.535568 \n",
       "L 132.2436 341.426168 \n",
       "L 134.792709 341.224637 \n",
       "L 137.341818 339.588317 \n",
       "L 139.890928 340.343911 \n",
       "L 142.440037 342.30106 \n",
       "L 144.989146 342.265474 \n",
       "L 147.538255 342.441742 \n",
       "L 150.087364 342.241256 \n",
       "L 152.636474 342.611518 \n",
       "L 155.185583 341.45706 \n",
       "L 157.734692 342.422163 \n",
       "L 162.83291 342.235503 \n",
       "L 167.931129 342.668981 \n",
       "L 170.480238 341.989564 \n",
       "L 173.029347 342.271152 \n",
       "L 175.578456 342.177061 \n",
       "L 178.127565 342.470846 \n",
       "L 180.676675 342.570698 \n",
       "L 183.225784 341.727618 \n",
       "L 185.774893 342.593496 \n",
       "L 188.324002 342.656802 \n",
       "L 190.873111 342.419614 \n",
       "L 193.42222 342.482161 \n",
       "L 195.97133 341.899817 \n",
       "L 198.520439 342.608061 \n",
       "L 201.069548 342.334108 \n",
       "L 203.618657 341.755431 \n",
       "L 206.167766 342.111452 \n",
       "L 208.716876 342.731361 \n",
       "L 211.265985 342.627378 \n",
       "L 213.815094 342.666145 \n",
       "L 216.364203 342.360048 \n",
       "L 218.913312 341.231742 \n",
       "L 221.462421 342.720437 \n",
       "L 224.011531 341.303208 \n",
       "L 226.56064 342.592482 \n",
       "L 229.109749 341.987581 \n",
       "L 231.658858 342.675455 \n",
       "L 236.757077 340.704091 \n",
       "L 239.306186 341.935288 \n",
       "L 241.855295 341.39737 \n",
       "L 244.404404 342.377029 \n",
       "L 246.953513 342.020495 \n",
       "L 249.502622 340.66603 \n",
       "L 252.051732 344.053912 \n",
       "L 254.600841 341.254642 \n",
       "L 257.14995 343.713251 \n",
       "L 259.699059 341.536216 \n",
       "L 262.248168 344.16159 \n",
       "L 264.797278 344.462382 \n",
       "L 267.346387 344.427311 \n",
       "L 269.895496 343.064918 \n",
       "L 272.444605 344.942796 \n",
       "L 274.993714 344.685913 \n",
       "L 280.091933 343.181395 \n",
       "L 282.641042 345.029703 \n",
       "L 285.190151 344.099843 \n",
       "L 287.73926 344.734037 \n",
       "L 290.288369 344.056272 \n",
       "L 292.837479 344.835396 \n",
       "L 295.386588 344.95865 \n",
       "L 297.935697 345.312537 \n",
       "L 300.484806 344.501329 \n",
       "L 303.033915 345.265442 \n",
       "L 305.583024 344.705207 \n",
       "L 308.132134 344.699538 \n",
       "L 310.681243 344.230408 \n",
       "L 315.779461 345.178687 \n",
       "L 318.32857 345.170278 \n",
       "L 320.87768 343.562617 \n",
       "L 323.426789 345.037419 \n",
       "L 325.975898 344.854096 \n",
       "L 328.525007 345.082858 \n",
       "L 331.074116 344.035026 \n",
       "L 333.623226 344.646428 \n",
       "L 336.172335 344.981582 \n",
       "L 338.721444 344.817446 \n",
       "L 341.270553 343.399116 \n",
       "L 343.819662 345.471388 \n",
       "L 346.368771 345.363441 \n",
       "L 348.917881 344.228358 \n",
       "L 351.46699 344.731462 \n",
       "L 354.016099 342.396441 \n",
       "L 356.565208 344.86456 \n",
       "L 359.114317 344.566832 \n",
       "L 361.663427 344.685608 \n",
       "L 364.212536 345.089149 \n",
       "L 369.310754 345.287838 \n",
       "L 371.859863 345.046707 \n",
       "L 374.408972 343.91032 \n",
       "L 376.958082 344.717508 \n",
       "L 379.507191 344.943987 \n",
       "L 382.0563 344.910013 \n",
       "L 384.605409 345.342267 \n",
       "L 387.154518 344.233442 \n",
       "L 389.703628 345.075234 \n",
       "L 394.801846 344.816028 \n",
       "L 397.350955 344.305064 \n",
       "L 399.900064 345.093469 \n",
       "L 402.449173 345.469904 \n",
       "L 404.998283 345.246073 \n",
       "L 407.547392 344.661696 \n",
       "L 410.096501 344.840413 \n",
       "L 412.64561 345.268617 \n",
       "L 415.194719 345.189231 \n",
       "L 417.743829 343.474319 \n",
       "L 420.292938 344.484101 \n",
       "L 422.842047 344.330193 \n",
       "L 425.391156 345.084826 \n",
       "L 427.940265 344.470228 \n",
       "L 430.489374 343.560548 \n",
       "L 433.038484 344.864522 \n",
       "L 435.587593 345.336716 \n",
       "L 438.136702 344.798583 \n",
       "L 440.685811 344.600078 \n",
       "L 443.23492 344.865156 \n",
       "L 445.78403 345.589069 \n",
       "L 448.333139 345.049604 \n",
       "L 450.882248 345.470776 \n",
       "L 453.431357 345.29581 \n",
       "L 455.980466 344.871867 \n",
       "L 458.529575 344.045774 \n",
       "L 461.078685 345.037126 \n",
       "L 463.627794 345.449737 \n",
       "L 466.176903 345.437403 \n",
       "L 468.726012 344.503363 \n",
       "L 471.275121 345.219899 \n",
       "L 473.824231 344.713943 \n",
       "L 476.37334 343.705219 \n",
       "L 478.922449 345.239953 \n",
       "L 481.471558 345.462348 \n",
       "L 484.020667 345.155957 \n",
       "L 486.569776 345.346704 \n",
       "L 489.118886 345.327788 \n",
       "L 491.667995 344.407414 \n",
       "L 496.766213 345.38125 \n",
       "L 499.315322 344.71721 \n",
       "L 501.864432 345.383492 \n",
       "L 506.96265 345.622126 \n",
       "L 509.511759 345.019852 \n",
       "L 512.060868 345.11329 \n",
       "L 514.609977 344.8033 \n",
       "L 517.159087 345.32873 \n",
       "L 519.708196 345.472588 \n",
       "L 522.257305 344.53023 \n",
       "L 524.806414 345.110237 \n",
       "L 527.355523 345.053939 \n",
       "L 529.904633 345.273292 \n",
       "L 532.453742 345.011964 \n",
       "L 535.002851 343.515863 \n",
       "L 537.55196 345.481395 \n",
       "L 540.101069 344.907745 \n",
       "L 542.650178 345.150711 \n",
       "L 545.199288 344.963883 \n",
       "L 547.748397 345.487761 \n",
       "L 550.297506 344.41009 \n",
       "L 552.846615 344.345131 \n",
       "L 555.395724 345.150385 \n",
       "L 560.493943 345.28333 \n",
       "L 563.043052 345.037945 \n",
       "L 565.592161 345.161037 \n",
       "L 568.14127 345.663889 \n",
       "L 570.690379 345.677721 \n",
       "L 573.239489 345.324689 \n",
       "L 573.239489 345.324689 \n",
       "\" clip-path=\"url(#pe9dde70291)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 354.958125 \n",
       "L 40.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 598.603125 354.958125 \n",
       "L 598.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 354.958125 \n",
       "L 598.603125 354.958125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 22.318125 \n",
       "L 598.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Training and validation loss -->\n",
       "    <g transform=\"translate(237.793125 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"46.333984\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"87.447266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"148.726562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"176.509766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"239.888672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"267.671875\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" x=\"331.050781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"394.527344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"426.314453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"487.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"550.972656\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"614.449219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-76\" x=\"646.236328\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"705.416016\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"766.695312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"794.478516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"822.261719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"885.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" x=\"947.017578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" x=\"986.226562\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1014.009766\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" x=\"1075.191406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"1138.570312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"1170.357422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"1198.140625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1259.322266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"1311.421875\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 512.0125 60.230625 \n",
       "L 591.603125 60.230625 \n",
       "Q 593.603125 60.230625 593.603125 58.230625 \n",
       "L 593.603125 29.318125 \n",
       "Q 593.603125 27.318125 591.603125 27.318125 \n",
       "L 512.0125 27.318125 \n",
       "Q 510.0125 27.318125 510.0125 29.318125 \n",
       "L 510.0125 58.230625 \n",
       "Q 510.0125 60.230625 512.0125 60.230625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 514.0125 35.416563 \n",
       "L 524.0125 35.416563 \n",
       "L 534.0125 35.416563 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(542.0125 38.916563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 514.0125 50.372813 \n",
       "L 524.0125 50.372813 \n",
       "L 534.0125 50.372813 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(542.0125 53.872813) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe9dde70291\">\n",
       "   <rect x=\"40.603125\" y=\"22.318125\" width=\"558\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(batched_history, 'loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
